# Ù¾ÛŒØ´ Ù†ÙˆÛŒØ³ Ù¾Ø±ÙˆÚ˜Ù‡

# 

## Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ

Ø§ÛŒÙ† Ø¯Ø§Ú©ÛŒÙˆÙ…Ù†Øª ÛŒÚ© Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Production-ready Ø¨Ø±Ø§ÛŒ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Enterprise-level Ø§ÛŒÙ†Ø³ØªØ§Ú¯Ø±Ø§Ù… Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ù‡Ø¯Ù Ù…Ø§ Ø³Ø§Ø®Øª Ø³ÛŒØ³ØªÙ…ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨ØªÙˆØ§Ù†Ø¯ **ØµØ¯Ù‡Ø§ Ù…ÛŒÙ„ÛŒÙˆÙ† Ù¾Ø³Øª** Ùˆ **Ø¯Ù‡Ù‡Ø§ Ù…ÛŒÙ„ÛŒÙˆÙ† ØµÙØ­Ù‡** Ø±Ø§ Ø¨Ø§ **Latency Ú©Ù…ØªØ± Ø§Ø² 300 Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡** Ùˆ **Throughput Ø¨Ø§Ù„Ø§ÛŒ 10,000 Query Per Second** Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†Ø¯.

### Ú†Ø±Ø§ Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§Ø³ØªØŸ

Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Social Media Ø¨Ø§ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ø§Ø³ÛŒ Ø¯Ø§Ø±Ø¯:


1. **Ø§ Scale**: Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§ (100M+ documents) Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ sharding Ùˆ distribution Ø¯Ø§Ø±Ø¯
2. **Ø§ Real-time**: Ù…Ø­ØªÙˆØ§ Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ú©Ù…ØªØ± Ø§Ø² 30 Ø«Ø§Ù†ÛŒÙ‡ Ù¾Ø³ Ø§Ø² publish Ù‚Ø§Ø¨Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§Ø´Ø¯
3. **Ø§ Relevance**: Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ relevant ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ù†Ø¯ØŒ Ù†Ù‡ ÙÙ‚Ø· keyword match
4. **Ø§ Multimodal**: Ø¨Ø§ÛŒØ¯ Ø¯Ø± text (caption)ØŒ OCR Ø§Ø² imagesØŒ transcription Ø§Ø² videos Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†ÛŒÙ…
5. **Ø§ Social Signals**: Engagement metrics (likes, comments) Ø¨Ø§ÛŒØ¯ Ø¯Ø± ranking ØªØ£Ø«ÛŒØ± Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯
6. **Ø§ Fresh Content**: Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§ØºÙ„Ø¨ relevant ØªØ± Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø§Ø³Øª (time decay)

### Ø§Ù‡Ø¯Ø§Ù Ù‚Ø§Ø¨Ù„ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ

| Metric | Target | ØªÙˆØ¶ÛŒØ­ |
|----|----|----|
| Query Latency (P95) | <300ms | 95% queries Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ú©Ù…ØªØ± Ø§Ø² 300ms Ù¾Ø§Ø³Ø® Ø¨Ú¯ÛŒØ±Ù†Ø¯ |
| Throughput | 10,000 QPS | ØªÙˆØ§Ù†Ø§ÛŒÛŒ handle Ú©Ø±Ø¯Ù† 10 Ù‡Ø²Ø§Ø± query Ø¯Ø± Ø«Ø§Ù†ÛŒÙ‡ |
| Indexing Latency | <30s | Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ÛŒØ¯ Ø¯Ø± 30 Ø«Ø§Ù†ÛŒÙ‡ searchable Ø´ÙˆØ¯ |
| Availability | 99.95% | Ø­Ø¯Ø§Ú©Ø«Ø± 4.38 Ø³Ø§Ø¹Øª downtime Ø¯Ø± Ø³Ø§Ù„ |
| NDCG@10 | >0.80 | Ú©ÛŒÙÛŒØª ranking (ØªÙˆØ¶ÛŒØ­ Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ø¨Ø®Ø´ metrics) |


---

## 1. Ø§Ù†ØªØ®Ø§Ø¨ Technology Stack

### 1.1 Ú†Ø±Ø§ ElasticsearchØŸ

**Ø¯Ù„Ø§ÛŒÙ„ Ø§ØµÙ„ÛŒ:**


1. **Ø§ Full-Text Search Native**: Elasticsearch Ø¨Ù‡ ØµÙˆØ±Øª native Ø¨Ø±Ø§ÛŒ full-text search Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø®Ù„Ø§Ù SQL databases Ú©Ù‡ full-text search ÛŒÚ© feature Ø§Ø¶Ø§ÙÛŒ Ø§Ø³ØªØŒ Elasticsearch Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡.
2. **Ø§ Horizontal Scalability**: Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† node Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ØŒ Ø¸Ø±ÙÛŒØª Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒÙ…. Elasticsearch Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± data Ø±Ø§ Ø¨ÛŒÙ† nodes ØªÙˆØ²ÛŒØ¹ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
3. **Ø§ Vector Search Support**: Ø§Ø² Ù†Ø³Ø®Ù‡ 8.x Ø¨Ù‡ Ø¨Ø¹Ø¯ØŒ Elasticsearch Ø§Ø² dense vector fields Ùˆ kNN search Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ semantic search Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.
4. **Ø§ Near Real-time**: Elasticsearch Ø¨Ø§ refresh interval 1 Ø«Ø§Ù†ÛŒÙ‡ (Ú©Ù‡ Ù…Ø§ Ø¢Ù† Ø±Ø§ 30 Ø«Ø§Ù†ÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)ØŒ near real-time indexing Ø¯Ø§Ø±Ø¯.
5. **Ø§ Rich Query DSL**: Query DSL Ø¨Ø³ÛŒØ§Ø± Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø§Ø³Øª Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… queries Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ø³Ø§Ø²ÛŒÙ….

**Alternatives Ùˆ Ú†Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù†Ø¯:**

* **Ø§ Apache Solr**: Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ØŒ community Ú©ÙˆÚ†Ú©â€ŒØªØ±ØŒ vector search Ø¶Ø¹ÛŒÙâ€ŒØªØ±
* **Ø§ Algolia**: Ø®Ø¯Ù…Ø§Øª managed ÙˆÙ„ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ pricing Ùˆ customization
* **Ø§ Meilisearch**: Ø¬Ø¯ÛŒØ¯ØŒ community Ú©ÙˆÚ†Ú©ØŒ feature Ù‡Ø§ÛŒ Ú©Ù…ØªØ±
* **Ø§ Typesense**: Ù…Ù†Ø§Ø³Ø¨ small-scaleØŒ Ø¨Ø±Ø§ÛŒ 100M+ documents proven Ù†ÛŒØ³Øª

```mermaidjs
graph TB
    subgraph "Ú†Ø±Ø§ ElasticsearchØŸ"
        A[Requirements] --> B{Scale}
        A --> C{Real-time}
        A --> D{Vector Search}
        A --> E{Complex Queries}
        
        B -->|100M+ docs| F[âœ“ Elasticsearch]
        C -->|<30s indexing| F
        D -->|Semantic search| F
        E -->|Bool + Script| F
        
        B -->|Limited| G[âœ— Algolia]
        C -->|Slow| H[âœ— Solr]
        D -->|Weak| H
    end
```

### 1.2 Ú†Ø±Ø§ SQL ServerØŸ

**Ø¯Ù„Ø§ÛŒÙ„ Ø§ØµÙ„ÛŒ:**


1. **Ø§ ACID Transactions**: Ø¨Ø±Ø§ÛŒ consistency Ùˆ data integrity Ú©Ù‡ Elasticsearch Ù†Ø¯Ø§Ø±Ø¯
2. **Ø§ Source of Truth**: Elasticsearch ÛŒÚ© search index Ø§Ø³ØªØŒ Ù†Ù‡ database. SQL Server Ù…Ù†Ø¨Ø¹ Ø§ØµÙ„ÛŒ data Ø§Ø³Øª
3. **Ø§Ø§ Structured Queries**: Ø¨Ø±Ø§ÛŒ analytics Ùˆ reportingØŒ SQL Ø¨Ø³ÛŒØ§Ø± Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ØªØ± Ø§Ø³Øª
4. **Ø§ Change Data Capture (CDC)**: SQL Server CDC Ù…Ø§ Ø±Ø§ Ù‚Ø§Ø¯Ø± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ ØªØºÛŒÛŒØ±Ø§Øª Ø±Ø§ track Ú©Ù†ÛŒÙ… Ùˆ Ø¨Ù‡ Elasticsearch sync Ú©Ù†ÛŒÙ…
5. **Ø§ Mature Ecosystem**: Backup/restoreØŒ monitoringØŒ tooling Ø¨Ø³ÛŒØ§Ø± mature Ø§Ø³Øª

**Ú†Ø±Ø§ Ù†Ù‡ PostgreSQLØŸ**

Ø§ PostgreSQL Ù‡Ù… Ú¯Ø²ÛŒÙ†Ù‡ Ø®ÙˆØ¨ÛŒ Ø§Ø³ØªØŒ Ø§Ù…Ø§:

* Ø§ SQL Server Ø¯Ø± Enterprise environments Ø¨ÛŒØ´ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
* Ø§ Integration Ø¨Ø§ .NET/C# Ø±Ø§Ø­Øªâ€ŒØªØ± Ø§Ø³Øª
* Ø§ TDE (Transparent Data Encryption) out-of-the-box
* Ù…Ø§ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªÛŒÙ… Ø¨Ø§ SQL Server familiar ØªØ± Ø§Ø³Øª

### 1.3 Ú†Ø±Ø§ Kafka + FlinkØŸ

**Kafka:**


1. **Ø§ Decoupling**: Producer Ùˆ Consumer Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ù‡Ø³ØªÙ†Ø¯
2. **Ø§ Durability**: Messages Ø¯Ø± disk persist Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ (7 Ø±ÙˆØ² retention)
3. **Ø§ Scalability**: Partitioning Ø¨Ø±Ø§ÛŒ parallel processing
4. **Ø§ Reliability**: Replication Ø¨Ø±Ø§ÛŒ fault tolerance

**Flink:**


1. **Ø§ Stateful Processing**: Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… state Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ… (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ deduplication)
2. **Ø§  Exactly-Once Semantics**: ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ù‡Ø± message Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ ÛŒÚ© Ø¨Ø§Ø± process Ø´ÙˆØ¯
3. **Ø§ Low Latency**: Ø¨Ø±Ø§ÛŒ real-time processing Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø³Øª
4. **Ø§ Rich Operators**: Window functionsØŒ aggregationsØŒ joins

 **Alternative: Apache Spark Streaming**

Ø§ Spark Streaming Ù‡Ù… Ø®ÙˆØ¨ Ø§Ø³Øª Ø§Ù…Ø§:

* Ø§ Micro-batch model Ø¯Ø§Ø±Ø¯ (latency Ø¨ÛŒØ´ØªØ±)
* Ø§ Flink Ø¨Ø±Ø§ÛŒ true streaming Ø¨Ù‡ØªØ± Ø§Ø³Øª
* Ø§ Flink state management Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ø³Øª

```mermaidjs
graph LR
    subgraph "Stream Processing Pipeline"
        A[Raw Data] -->|Ingest| B[Kafka Topics]
        B -->|Consume| C[Flink Jobs]
        C -->|Transform| D[Processed Data]
        D -->|Dual Write| E[SQL Server]
        D -->|Dual Write| F[Elasticsearch]
        
        style C fill:#f9f,stroke:#333,stroke-width:2px
    end
```

### 1.4 Ú†Ø±Ø§ Redis Ø¨Ø±Ø§ÛŒ CachingØŸ


1. **In-Memory**: Sub-millisecond latency
2. **Data Structures**: Support Ø¨Ø±Ø§ÛŒ StringØŒ HashØŒ ListØŒ Set (Ù…Ø§ String Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)
3. **TTL Support**: Automatic expiration
4. **Distributed**: Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Redis Cluster Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…
5. **Persistence**: RDB snapshots + AOF Ø¨Ø±Ø§ÛŒ durability

**Alternative: Memcached**

Memcached Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø§Ø³Øª Ø§Ù…Ø§:

* TTL per key Ù†Ø¯Ø§Ø±Ø¯ (ÙÙ‚Ø· global)
* Data structures Ù†Ø¯Ø§Ø±Ø¯
* Persistence Ù†Ø¯Ø§Ø±Ø¯
* Redis feature-rich ØªØ± Ø§Ø³Øª


---

## 2. Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ø¨Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª

### 2.1 Ù†Ú¯Ø§Ù‡ Ú©Ù„ÛŒ (High-Level Architecture)

```mermaidjs
graph TB
    subgraph "Client Layer"
        CL1[Web App]
        CL2[Mobile App]
        CL3[API Clients]
    end
    
    subgraph "API Gateway"
        AG[NGINX/ALB]
        RL[Rate Limiter]
        AUTH[Auth Service]
    end
    
    subgraph "Application Layer"
        SS[Search Service]
        IS[Ingestion Service]
        ES[Embedding Service]
    end
    
    subgraph "Caching Layer"
        RC[Redis Cluster]
    end
    
    subgraph "Search Layer"
        ESC[Elasticsearch Cluster]
    end
    
    subgraph "Streaming Layer"
        KC[Kafka Cluster]
        FL[Flink Processor]
    end
    
    subgraph "Data Layer"
        SQL[SQL Server]
    end
    
    CL1 & CL2 & CL3 --> AG
    AG --> RL --> AUTH
    AUTH --> SS
    AUTH --> IS
    
    SS --> RC
    RC -.Cache Miss.-> ESC
    SS --> ESC
    
    IS --> KC
    KC --> FL
    FL --> SQL
    FL --> ESC
    
    SQL -.CDC.-> FL
    
    style SS fill:#bbf,stroke:#333,stroke-width:2px
    style ESC fill:#bfb,stroke:#333,stroke-width:2px
    style SQL fill:#fbb,stroke:#333,stroke-width:2px
```

### 2.2 ØªÙˆØ¶ÛŒØ­ Ø¬Ø²Ø¦ÛŒØ§Øª Ù‡Ø± Layer

#### 2.2.1 API Gateway Layer

**Ù…Ø³Ø¦ÙˆÙ„ÛŒØª:** First line of defense Ùˆ entry point ÙˆØ§Ø­Ø¯ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… requests.

**Ú†Ø±Ø§ NGINX ÛŒØ§ AWS ALBØŸ**

* **NGINX**:
  * Open-sourceØŒ Ø±Ø§ÛŒÚ¯Ø§Ù†
  * Performance Ø¨Ø§Ù„Ø§ (100K+ requests/sec)
  * Flexible configuration
  * ÙˆØ§Ø³Ø· Ø¨Ø±Ø§ÛŒ rate limitingØŒ circuit breaking
* **AWS ALB (Application Load Balancer)**:
  * Managed service (Ú©Ù…ØªØ± maintenance)
  * Auto-scaling
  * Health checks integrated
  * SSL/TLS termination

**Rate Limiting Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ**

```mermaidjs
sequenceDiagram
    participant Client
    participant RateLimiter
    participant Redis
    participant Backend
    
    Client->>RateLimiter: Request
    RateLimiter->>Redis: INCR user:123:count
    Redis-->>RateLimiter: Current count (45)
    
    alt Count <= Limit (100)
        RateLimiter->>Redis: EXPIRE user:123:count 60
        RateLimiter->>Backend: Forward request
        Backend-->>RateLimiter: Response
        RateLimiter-->>Client: 200 OK
    else Count > Limit
        RateLimiter-->>Client: 429 Too Many Requests
    end
```

**ØªÙˆØ¶ÛŒØ­ Algorithm:**

Ù…Ø§ Ø§Ø² **Sliding Window Counter** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:


1. Ù‡Ø± user ÛŒÚ© key Ø¯Ø± Redis Ø¯Ø§Ø±Ø¯: `rate_limit:user:{user_id}:{timestamp_minute}`
2. Ø¨Ø§ Ù‡Ø± requestØŒ counter Ø±Ø§ increment Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
3. Ø§Ú¯Ø± counter Ø¨ÛŒØ´ØªØ± Ø§Ø² limit Ø´Ø¯ØŒ request Ø±Ø§ reject Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
4. TTL Ø±ÙˆÛŒ key Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ… ØªØ§ Ø¨Ø¹Ø¯ Ø§Ø² 1 Ø¯Ù‚ÛŒÙ‚Ù‡ expire Ø´ÙˆØ¯

**Ú†Ø±Ø§ Ù†Ù‡ Fixed WindowØŸ**

Fixed Window ÛŒÚ© Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ø¯: Ø¯Ø± edge of window Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ 2Ã— traffic Ø¨ÙØ±Ø³ØªÙ†Ø¯.

Ù…Ø«Ø§Ù„:

* Limit = 100 req/min
* 00:00:50 - 00:00:59: 100 request
* 00:01:00 - 00:01:09: 100 request
* Ø¯Ø± 20 Ø«Ø§Ù†ÛŒÙ‡ØŒ 200 request! (burst)

Sliding Window Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯.

#### 2.2.2 Search Service (Core Business Logic)

**Architecture Pattern: Layered Architecture**

```mermaidjs
graph TB
    subgraph "Search Service Layers"
        API[API Layer - Controllers]
        BL[Business Logic - Services]
        DL[Data Access - Repositories]
        
        API --> BL
        BL --> DL
        
        subgraph "Cross-Cutting Concerns"
            LOG[Logging]
            MON[Monitoring]
            CACHE[Caching]
        end
        
        API -.-> LOG
        BL -.-> MON
        DL -.-> CACHE
    end
```

**Ú†Ø±Ø§ Singleton ElasticClientØŸ**

```csharp
// âŒ BAD: Creating client per request
public class SearchService
{
    public async Task<Result> SearchAsync(string query)
    {
        var client = new ElasticClient(...); // Ù‡Ø± Ø¨Ø§Ø± client Ø¬Ø¯ÛŒØ¯!
        return await client.SearchAsync(...);
    }
}

// âœ… GOOD: Singleton pattern
public class SearchService
{
    private readonly IElasticClient _client; // ÛŒÚ© instance Ø¯Ø± Ú©Ù„ app
    
    public SearchService(IElasticClient client)
    {
        _client = client; // Injected via DI
    }
}
```

**Ø¯Ù„ÛŒÙ„:**


1. **Connection Pooling**: ElasticClient Ø¯Ø§Ø®Ù„Ø§Ù‹ connection pool Ø¯Ø§Ø±Ø¯. Ø§Ú¯Ø± Ù‡Ø± Ø¨Ø§Ø± instance Ø¬Ø¯ÛŒØ¯ Ø¨Ø³Ø§Ø²ÛŒÙ…ØŒ Ø§ÛŒÙ† pool Ø§Ø² Ø¨ÛŒÙ† Ù…ÛŒâ€ŒØ±ÙˆØ¯.
2. **Memory**: Ù‡Ø± ElasticClient Ú†Ù†Ø¯ MB memory Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯. Ø¨Ø§ 10,000 req/secØŒ Ø§Ú¯Ø± Ù‡Ø± request ÛŒÚ© client Ø¨Ø³Ø§Ø²Ø¯ØŒ memory exhaustion Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø´Øª.
3. **Performance**: Ø³Ø§Ø®Øª client overhead Ø¯Ø§Ø±Ø¯ (DNS lookupØŒ connection establishment).

**Thread Safety:**

ElasticClient thread-safe Ø§Ø³Øª. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ù‡Ù…Ø²Ù…Ø§Ù† Ø§Ø² multiple threads Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ….

#### 2.2.3 Elasticsearch Cluster

**Ú†Ú¯ÙˆÙ†Ù‡ Sharding Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ**

```mermaidjs
graph TB
    subgraph "Document Routing"
        DOC[Document: post_id=12345]
        HASH[Hash Function]
        SHARD[Shard = hash % num_shards]
        
        DOC --> HASH
        HASH -->|hash = 54321| SHARD
        SHARD -->|54321 % 18 = 15| S15[Shard 15]
    end
    
    subgraph "Shard Distribution"
        S15 --> N1[Node 1 - Primary]
        S15 --> N2[Node 2 - Replica]
    end
```

**Ú†Ø±Ø§ 18 ShardsØŸ**

Ù…Ø­Ø§Ø³Ø¨Ù‡:

* Total data: 100M posts Ã— 5KB = 500GB
* Optimal shard size: 10-50GB (Elasticsearch recommendation)
* 500GB Ã· 18 = 27.7GB per shard âœ“

**Ú†Ø±Ø§ Ù†Ù‡ 10 Shards ÛŒØ§ 30 ShardsØŸ**

* **Ú©Ù…ØªØ± Ø§Ø² 10**: Ù‡Ø± shard Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ø§Ø² 50GB Ù…ÛŒâ€ŒØ´ÙˆØ¯ â†’ slow performance
* **Ø¨ÛŒØ´ØªØ± Ø§Ø² 30**: overhead of coordination Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ±ÙˆØ¯ â†’ slow searches

**Replication Factor = 1**

ÛŒØ¹Ù†ÛŒ Ù‡Ø± shard ÛŒÚ© replica Ø¯Ø§Ø±Ø¯:

* Primary shard: Ø±ÙˆÛŒ node 1
* Replica shard: Ø±ÙˆÛŒ node 2

Ø§Ú¯Ø± node 1 down Ø´ÙˆØ¯ØŒ node 2 Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

**Over-Sharding Problem:**

Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ shards Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯ Ø¨Ø§Ø´Ø¯:


1. Memory overhead: Ù‡Ø± shard Ø­Ø§ÙØ¸Ù‡ Ù…ØµØ±Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯
2. Coordination cost: Elasticsearch Ø¨Ø§ÛŒØ¯ Ù†ØªØ§ÛŒØ¬ Ø§Ø² Ù‡Ù…Ù‡ shards Ø±Ø§ merge Ú©Ù†Ø¯
3. Scatter-gather queries: query Ø¨Ù‡ Ù‡Ù…Ù‡ shards Ù…ÛŒâ€ŒØ±ÙˆØ¯ (broadcast)

**Under-Sharding Problem:**

Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ shards Ø®ÛŒÙ„ÛŒ Ú©Ù… Ø¨Ø§Ø´Ø¯:


1. Large shard size â†’ slow indexing/searching
2. Hot shards: Ø¨Ø±Ø®ÛŒ shards Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨ÛŒØ´ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯
3. Limited parallelism: Ú©Ù…ØªØ± parallel search Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Index Lifecycle Management (ILM):**

```mermaidjs
stateDiagram-v2
    [*] --> Hot: New document
    Hot --> Warm: After 30 days
    Warm --> Cold: After 90 days
    Cold --> Delete: After 365 days
    
    note right of Hot
        Fast SSD
        Heavy indexing
        Frequent searches
    end note
    
    note right of Warm
        Shrink shards
        Force merge
        Reduce replicas
    end note
    
    note right of Cold
        Slow storage
        Freeze index
        Searchable snapshot
    end note
```

**ØªÙˆØ¶ÛŒØ­:**

* **Hot tier**: Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯ØŒ Ø±ÙˆÛŒ SSD Ø³Ø±ÛŒØ¹ØŒ indexing Ù…Ú©Ø±Ø±
* **Warm tier**: Ù…Ø­ØªÙˆØ§ÛŒ 1 Ù…Ø§Ù‡Ù‡ØŒ shrink Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (18 shard â†’ 6 shard)ØŒ force merge
* **Cold tier**: Ù…Ø­ØªÙˆØ§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ØŒ Ø±ÙˆÛŒ HDD Ø§Ø±Ø²Ø§Ù†â€ŒØªØ±ØŒ frozen (searchable snapshot)
* **Delete**: Ù…Ø­ØªÙˆØ§ÛŒ Ø¨ÛŒØ´ Ø§Ø² 1 Ø³Ø§Ù„ Ø±Ø§ Ù¾Ø§Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…

**Ú†Ø±Ø§ Ø§ÛŒÙ† approachØŸ**


1. **Cost Optimization**: Storage cost Ø¨Ø±Ø§ÛŒ old data Ú©Ù…ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯
2. **Performance**: Hot tier Ø¨Ø±Ø§ÛŒ new data fast Ø§Ø³Øª
3. **Compliance**: Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… old data Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ… (legal requirements)

#### 2.2.4 SQL Server Ø´ÛŒÙ…Ø§ Ùˆ Optimizations

**Ú†Ø±Ø§ NormalizationØŸ**

```sql
-- âŒ Denormalized (Bad for updates)
CREATE TABLE Posts (
    post_id BIGINT PRIMARY KEY,
    hashtag1 VARCHAR(100),
    hashtag2 VARCHAR(100),
    hashtag3 VARCHAR(100),
    ...
);

-- âœ… Normalized (Good for consistency)
CREATE TABLE Posts (
    post_id BIGINT PRIMARY KEY,
    caption NVARCHAR(MAX)
);

CREATE TABLE Hashtags (
    hashtag_id BIGINT PRIMARY KEY,
    hashtag NVARCHAR(255) UNIQUE
);

CREATE TABLE PostHashtags (
    post_id BIGINT REFERENCES Posts(post_id),
    hashtag_id BIGINT REFERENCES Hashtags(hashtag_id),
    PRIMARY KEY (post_id, hashtag_id)
);
```

**Ø¯Ù„Ø§ÛŒÙ„:**


1. **No Duplication**: Hashtag "#sunset" ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
2. **Easy Updates**: Ø§Ú¯Ø± Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ… hashtag Ø±Ø§ rename Ú©Ù†ÛŒÙ…ØŒ ÙÙ‚Ø· ÛŒÚ© Ø¬Ø§ Ø¹ÙˆØ¶ Ù…ÛŒâ€ŒØ´ÙˆØ¯
3. **Constraints**: Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… foreign key constraints Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…

**Indexing Strategy:**

```sql
-- Clustered Index (physical order)
CREATE CLUSTERED INDEX IX_Posts_PostId ON Posts(post_id);

-- Non-Clustered Indexes (logical pointers)
CREATE NONCLUSTERED INDEX IX_Posts_Username ON Posts(username);
CREATE NONCLUSTERED INDEX IX_Posts_Created ON Posts(created_at DESC);
CREATE NONCLUSTERED INDEX IX_Posts_Engagement 
    ON Posts(engagement_score DESC) 
    INCLUDE (username, caption);
```

**ØªÙˆØ¶ÛŒØ­ Clustered vs Non-Clustered:**

* **Clustered Index**: ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ data ÙÛŒØ²ÛŒÚ©Ø§Ù‹ Ú†Ú¯ÙˆÙ†Ù‡ Ø±ÙˆÛŒ disk Ø°Ø®ÛŒØ±Ù‡ Ø´ÙˆØ¯. ÙÙ‚Ø· **ÛŒÚ©** clustered index Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…. Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø±ÙˆÛŒ Primary Key Ø§Ø³Øª.
* **Non-Clustered Index**: ÛŒÚ© structure Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø§Ø³Øª Ú©Ù‡ pointer Ø¨Ù‡ clustered index Ø¯Ø§Ø±Ø¯. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… **Ú†Ù†Ø¯** non-clustered index Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ….

**INCLUDE Clause Ú†ÛŒØ³ØªØŸ**

```sql
CREATE INDEX IX_Posts_Engagement 
    ON Posts(engagement_score DESC) 
    INCLUDE (username, caption);
```

Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ:

* Index Ø¨Ø± Ø§Ø³Ø§Ø³ `engagement_score` sort Ù…ÛŒâ€ŒØ´ÙˆØ¯
* ÙˆÙ„ÛŒ `username` Ùˆ `caption` Ù‡Ù… Ø¯Ø± index Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
* ÙˆÙ‚ØªÛŒ query ÙÙ‚Ø· Ø§ÛŒÙ† columns Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯ØŒ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø±ÙØªÙ† Ø¨Ù‡ main table Ù†ÛŒØ³Øª (covering index)

**Ù…Ø«Ø§Ù„:**

```sql
-- Ø§ÛŒÙ† query Ø§Ø² index Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ table lookup Ù†Ø¯Ø§Ø±Ø¯
SELECT username, caption
FROM Posts
WHERE engagement_score > 1000
ORDER BY engagement_score DESC;
```

**Change Data Capture (CDC):**

CDC Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ

```mermaidjs
sequenceDiagram
    participant App
    participant SQL
    participant CDCTable
    participant Flink
    participant ES
    
    App->>SQL: UPDATE Posts SET likes=100 WHERE id=1
    SQL->>SQL: Write to transaction log
    SQL->>CDCTable: Capture change
    
    Note over CDCTable: Change table:<br/>op=UPDATE<br/>id=1<br/>likes=100
    
    Flink->>CDCTable: Poll for changes
    CDCTable-->>Flink: Return changes
    Flink->>ES: Update document
```

**Ù…Ø²Ø§ÛŒØ§:**


1. **Low Overhead**: CDC Ø§Ø² transaction log Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù¾Ø±Ø³ Ùˆ Ø¬ÙˆÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ù†Ø¯Ø§Ø±Ø¯
2. **All Changes**: Ù‡Ù…Ù‡ INSERTØŒ UPDATEØŒ DELETE capture Ù…ÛŒâ€ŒØ´ÙˆØ¯
3. **Ordering**: ØªØ±ØªÛŒØ¨ changes Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯
4. **No Code Changes**: Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ ØªØºÛŒÛŒØ± application code Ù†ÛŒØ³Øª


---

## 3. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ - ØªÙˆØ¶ÛŒØ­ Ú©Ø§Ù…Ù„

### 3.1 BM25 Algorithm (Best Matching 25)

**ÙØ±Ù…ÙˆÙ„ Ø±ÛŒØ§Ø¶ÛŒ:**

$$
\text{BM25}(q, d) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, d) \cdot (k_1 + 1)}{f(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}
$$

**Ø§Ø¬Ø²Ø§ÛŒ ÙØ±Ù…ÙˆÙ„:**


1. **IDF (Inverse Document Frequency)**:

   $$
   \text{IDF}(q_i) = \log\left(\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1\right)
   $$
   * $N$: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ documents
   * $n(q_i)$: ØªØ¹Ø¯Ø§Ø¯ documents Ú©Ù‡ term $q_i$ Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯

   **Ù…Ø¹Ù†ÛŒ:** Terms Ú©Ù‡ Ø¯Ø± Ú©Ù…ØªØ± documents Ù‡Ø³ØªÙ†Ø¯ØŒ Ù…Ù‡Ù…â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯.

   **Ù…Ø«Ø§Ù„:**
   * Query: "best coffee shop"
   * "best": Ø¯Ø± 50M documents Ø§Ø³Øª â†’ IDF Ú©Ù…
   * "coffee": Ø¯Ø± 5M documents Ø§Ø³Øª â†’ IDF Ù…ØªÙˆØ³Ø·
   * "shop": Ø¯Ø± 10M documents Ø§Ø³Øª â†’ IDF Ù…ØªÙˆØ³Ø·

   Ù¾Ø³ "coffee" Ùˆ "shop" Ø¨ÛŒØ´ØªØ± Ø§Ø² "best" Ø¯Ø± scoring ØªØ£Ø«ÛŒØ± Ø¯Ø§Ø±Ù†Ø¯.
2. **Term Frequency (TF)**:

   $$
   \text{TF} = \frac{f(q_i, d) \cdot (k_1 + 1)}{f(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}
   $$
   * $f(q_i, d)$: ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§ØªÛŒ Ú©Ù‡ term Ø¯Ø± document Ø¸Ø§Ù‡Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯
   * $k_1$: tuning parameter (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 1.2 - 2.0)
   * $b$: length normalization (0 - 1)
   * $|d|$: Ø·ÙˆÙ„ document (ØªØ¹Ø¯Ø§Ø¯ terms)
   * $\text{avgdl}$: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ documents

**Ú†Ø±Ø§ Ø§ÛŒÙ† ÙØ±Ù…ÙˆÙ„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§Ø³ØªØŸ**

ÙØ±Ù…ÙˆÙ„ Ø³Ø§Ø¯Ù‡ TF-IDF:

$$
\text{score} = \text{TF} \times \text{IDF}
$$

Ù…Ø´Ú©Ù„: Ø§Ú¯Ø± ÛŒÚ© term 100 Ø¨Ø§Ø± Ø¯Ø± document Ø¨Ø§Ø´Ø¯ØŒ score Ø®ÛŒÙ„ÛŒ Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (keyword stuffing).

Ø§ BM25 Ø§ÛŒÙ† Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¨Ø§ **saturation**: Ø¨Ø¹Ø¯ Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¹ÛŒÙ†ÛŒØŒ Ø§ÙØ²Ø§ÛŒØ´ TF ØªØ£Ø«ÛŒØ± Ú©Ù…ÛŒ Ø¯Ø§Ø±Ø¯.

```python
import matplotlib.pyplot as plt
import numpy as np

# Simulate TF effect
tf_values = np.arange(0, 50, 1)
k1 = 1.5

# Simple TF (linear)
simple_tf = tf_values

# BM25 TF (saturating)
bm25_tf = (tf_values * (k1 + 1)) / (tf_values + k1)

plt.plot(tf_values, simple_tf, label='Simple TF (Linear)')
plt.plot(tf_values, bm25_tf, label='BM25 TF (Saturating)')
plt.xlabel('Term Frequency')
plt.ylabel('Score Contribution')
plt.legend()
plt.title('BM25 Saturation Effect')
```

**Output:** Ø¨Ø¹Ø¯ Ø§Ø² TF=10ØŒ BM25 score Ø®ÛŒÙ„ÛŒ Ú©Ù… Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ØŒ ÙˆÙ„ÛŒ simple TF Ø®Ø·ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

**Length Normalization (**$b$ **parameter):**

```mermaid
graph LR
    A[Document Length] --> B{b = 0}
    A --> C{b = 0.75}
    A --> D{b = 1}
    
    B --> E[No normalization<br/>Long docs favored]
    C --> F[Partial normalization<br/>Balanced]
    D --> G[Full normalization<br/>Short docs favored]
```

**Ú†Ø±Ø§ b=0.75 Ø¨Ø±Ø§ÛŒ Social MediaØŸ**

* Captions Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ú©ÙˆØªØ§Ù‡ Ù‡Ø³ØªÙ†Ø¯ (50-200 Ú©Ø§Ø±Ø§Ú©ØªØ±)
* Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… captions Ø·ÙˆÙ„Ø§Ù†ÛŒ penalize Ù†Ø´ÙˆÙ†Ø¯ØŒ ÙˆÙ„ÛŒ keyword stuffing Ù‡Ù… prevent Ø´ÙˆØ¯
* b=0.75 ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª

**Field Boosting:**

```json
{
  "query": {
    "multi_match": {
      "query": "sunset beach",
      "fields": [
        "caption^5",      // 5x weight
        "hashtags^3",     // 3x weight
        "ocr_text^2",     // 2x weight
        "comments^1"      // 1x weight
      ]
    }
  }
}
```

**Ù…Ù†Ø·Ù‚:**


1. **caption^5**: Caption Ø§ØµÙ„ÛŒâ€ŒØªØ±ÛŒÙ† Ù…Ø­ØªÙˆØ§ Ø§Ø³Øª â†’ ÙˆØ²Ù† Ø¨Ø§Ù„Ø§
2. **hashtags^3**: Hashtags Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ù‡Ø³ØªÙ†Ø¯ â†’ ÙˆØ²Ù† Ù…ØªÙˆØ³Ø·
3. **ocr_text^2**: OCR Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø®Ø·Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ â†’ ÙˆØ²Ù† Ú©Ù…ØªØ±
4. **comments^1**: Comments Ù…Ù…Ú©Ù† Ø§Ø³Øª off-topic Ø¨Ø§Ø´Ù†Ø¯ â†’ ÙˆØ²Ù† Ù¾Ø§ÛŒÙ‡

### 3.2 Vector Search Ø¨Ø§ HNSW

**Ú†Ø±Ø§ Vector SearchØŸ**

Ø§ BM25 ÙÙ‚Ø· exact keyword match Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…Ø«Ø§Ù„:

* Query: "cute puppy"
* Document 1: "adorable dog" â† BM25 score = 0 (no match)
* Document 2: "cute puppy" â† BM25 score = high

ÙˆÙ„ÛŒ Document 1 semantically relevant Ø§Ø³Øª!

Vector search Ø§ÛŒÙ† Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¨Ø§ **embeddings**.

**Embeddings Ú†ÛŒØ³ØªØŸ**

ØªØ¨Ø¯ÛŒÙ„ text Ø¨Ù‡ vector Ø¯Ø± ÙØ¶Ø§ÛŒ high-dimensional:

```
"cute puppy" â†’ [0.23, -0.45, 0.12, ..., 0.67]  (384 dimensions)
"adorable dog" â†’ [0.21, -0.43, 0.14, ..., 0.65]
```

Ø§ÛŒÙ† vectors Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ù‡Ù… Ù‡Ø³ØªÙ†Ø¯ (cosine similarity â‰ˆ 0.92).

**Ú†Ú¯ÙˆÙ†Ù‡ Embeddings generate Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŸ**

Ù…Ø§ Ø§Ø² **Sentence Transformers** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L12-v2')

# Generate embedding
text = "Beautiful sunset at the beach"
embedding = model.encode(text)
print(embedding.shape)  # (384,)
```

**Ú†Ø±Ø§ Ø§ÛŒÙ† modelØŸ**

* **Size**: 33M parameters (Ø³Ø¨Ú©ØŒ Ø³Ø±ÛŒØ¹)
* **Dimensions**: 384 (balance Ø¨ÛŒÙ† accuracy Ùˆ speed)
* **Quality**: SOTA performance Ø±ÙˆÛŒ semantic similarity tasks
* **Multilingual**: Support Ø¨Ø±Ø§ÛŒ 50+ Ø²Ø¨Ø§Ù†

**Alternatives:**

| Model | Dimensions | Performance | Speed |
|----|----|----|----|
| all-MiniLM-L12-v2 | 384 | Good | Fast |
| all-mpnet-base-v2 | 768 | Better | Slower |
| BERT-base | 768 | Good | Slow |
| USE (Universal Sentence Encoder) | 512 | Good | Medium |

**HNSW Algorithm (Hierarchical Navigable Small World)**

HNSW ÛŒÚ© graph-based algorithm Ø¨Ø±Ø§ÛŒ Approximate Nearest Neighbor (ANN) search Ø§Ø³Øª.

```mermaidjs
graph TB
    subgraph "HNSW Layers"
        L0[Layer 0 - All Points]
        L1[Layer 1 - Subset]
        L2[Layer 2 - Smaller Subset]
        
        L2 -.Navigate.-> L1
        L1 -.Navigate.-> L0
    end
    
    subgraph "Search Process"
        Q[Query Vector]
        Q -->|1. Start at top layer| L2
        L2 -->|2. Find closest| N1[Node 1]
        N1 -->|3. Descend| L1
        L1 -->|4. Find closer| N2[Node 2]
        N2 -->|5. Descend| L0
        L0 -->|6. Final neighbors| R[Results]
    end
```

**Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ**


1. **Build Phase:**
   * Graph Ú†Ù†Ø¯ Ù„Ø§ÛŒÙ‡ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ…
   * Ù‡Ø± node Ø¨Ù‡ `M` neighbors Ù…ØªØµÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
   * Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ sparse Ù‡Ø³ØªÙ†Ø¯ (navigational)
   * Ù„Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ† dense Ø§Ø³Øª (exhaustive)
2. **Search Phase:**
   * Ø§Ø² Ù„Ø§ÛŒÙ‡ Ø¨Ø§Ù„Ø§ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
   * greedy search ØªØ§ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† node
   * Ø¨Ù‡ Ù„Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ† Ù…ÛŒâ€ŒØ±ÙˆÛŒÙ…
   * Ø§ÛŒÙ† Ø±Ø§ ØªÚ©Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
   * Ø¯Ø± Ù„Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ†ØŒ k Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† neighbors Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…

**Parameters:**

```json
{
  "index_options": {
    "type": "int8_hnsw",
    "m": 16,               // connections per layer
    "ef_construction": 100 // candidates during build
  }
}
```

**Trade-offs:**

| Parameter | â†‘ Ø²ÛŒØ§Ø¯ Ú©Ø±Ø¯Ù† | â†“ Ú©Ù… Ú©Ø±Ø¯Ù† |
|----|----|----|
| **m** | + Accuracy<br>- Build time<br>- Memory | + Speed<br>+ Memory<br>- Accuracy |
| **ef_construction** | + Accuracy<br>- Build time | + Speed<br>- Accuracy |

**Query-time Parameter:**

```json
{
  "knn": {
    "field": "caption_vector",
    "query_vector": [...],
    "k": 100,
    "num_candidates": 500  // ef_search
  }
}
```

**ef_search (num_candidates):**

* ØªØ¹Ø¯Ø§Ø¯ candidates Ú©Ù‡ explore Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
* Ù‡Ø±Ú†Ù‡ Ø¨ÛŒØ´ØªØ± â†’ accuracy Ø¨ÛŒØ´ØªØ±ØŒ ÙˆÙ„ÛŒ slow ØªØ±
* Rule of thumb: 5Ã— - 10Ã— k

**int8 Quantization Ú†ÛŒØ³ØªØŸ**

Quantization ÛŒØ¹Ù†ÛŒ ØªØ¨Ø¯ÛŒÙ„ float32 â†’ int8:

```
float32: [-0.456, 0.234, -0.891, ...]  (4 bytes per dim)
int8:    [-117, 60, -228, ...]          (1 byte per dim)
```

**Ù…Ø²Ø§ÛŒØ§:**

* **4Ã— Ú©Ù…ØªØ± memory**: 384 dims Ã— 4 bytes = 1536 bytes â†’ 384 bytes
* **Faster computation**: CPU operations Ø±ÙˆÛŒ int Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø³Øª

**Ù…Ø¹Ø§ÛŒØ¨:**

* **Precision loss**: Ø§Ø² 32-bit Ø¨Ù‡ 8-bit Ù…ÛŒâ€ŒØ±ÙˆÛŒÙ…
* **Slight accuracy drop**: Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ <2% drop Ø¯Ø± recall

**Ú†Ú¯ÙˆÙ†Ù‡ quantize Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŸ**

```python
# Original float32 vector
vector_f32 = np.array([0.234, -0.456, 0.891, ...])

# Quantize to int8
min_val, max_val = vector_f32.min(), vector_f32.max()
scale = (max_val - min_val) / 255
vector_i8 = ((vector_f32 - min_val) / scale).astype(np.int8)

# Dequantize (for similarity computation)
vector_restored = vector_i8 * scale + min_val
```

**Cosine Similarity vs Dot Product:**

```python
import numpy as np

v1 = np.array([1, 2, 3])
v2 = np.array([2, 3, 4])

# Cosine similarity
cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
print(cos_sim)  # 0.992

# Dot product
dot_prod = np.dot(v1, v2)
print(dot_prod)  # 20
```

**Ú†Ø±Ø§ cosineØŸ**

* **Normalized**: Ù…Ø³ØªÙ‚Ù„ Ø§Ø² magnitude Ø§Ø³Øª
* **Range**: \[-1, 1\] â†’ easy to interpret
* **Semantic**: Ø¨Ø±Ø§ÛŒ sentence embeddings Ø¨Ù‡ØªØ± Ø§Ø³Øª

**Ú†Ø±Ø§ Ù†Ù‡ Euclidean DistanceØŸ**

```python
# Euclidean distance
euclidean = np.linalg.norm(v1 - v2)
```

* Ø¨Ø±Ø§ÛŒ high-dimensional vectorsØŒ Euclidean distance Ú©Ù…ØªØ± discriminative Ø§Ø³Øª
* Cosine ÙÙ‚Ø· Ø¨Ù‡ direction ØªÙˆØ¬Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù†Ù‡ magnitude

### 3.3 Hybrid Search - ØªØ±Ú©ÛŒØ¨ BM25 + Vector

**Ú†Ø±Ø§ HybridØŸ**

| Approach | Pros | Cons |
|----|----|----|
| **BM25 Only** | â€¢ Fast<br>â€¢ Exact match<br>â€¢ Explainable | â€¢ No semantic understanding<br>â€¢ Vocabulary mismatch |
| **Vector Only** | â€¢ Semantic understanding<br>â€¢ Synonyms | â€¢ Slow (kNN)<br>â€¢ May return irrelevant results |
| **Hybrid** | â€¢ Best of both<br>â€¢ Robust | â€¢ Complexity<br>â€¢ Tuning needed |

**Ú†Ú¯ÙˆÙ†Ù‡ Combine Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŸ**

Ø¯Ùˆ approach Ø§ØµÙ„ÛŒ:


1. **Linear Combination:**

   $$
   \text{score} = \alpha \cdot \text{BM25} + (1-\alpha) \cdot \text{Vector}
   $$
2. **Rank Fusion:** Reciprocal Rank Fusion (RRF):

   $$
   \text{RRF}(d) = \sum_{r \in R} \frac{1}{k + r(d)}
   $$

   Ú©Ù‡ $r(d)$ rank of document $d$ Ø¯Ø± ranking $r$ Ø§Ø³Øª.

**Ù…Ø§ Ø§Ø² Ú†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…?**

Ù…Ø§ Ø§Ø² **Weighted Score Combination** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

```
Final Score = 0.30 Ã— BM25 +
              0.25 Ã— Vector +
              0.20 Ã— Engagement +
              0.15 Ã— Recency +
              0.10 Ã— Authority
```

**Ú†Ø±Ø§ Ø§ÛŒÙ† weightsØŸ**

Ø§ÛŒÙ† weights Ø§Ø² A/B testing Ùˆ optimization Ø¨Ø¯Ø³Øª Ø¢Ù…Ø¯Ù‡â€ŒØ§Ù†Ø¯:

```mermaid
graph TD
    subgraph "Weight Optimization Process"
        A[Initial Weights<br/>Equal: 0.2 each]
        B[Run A/B Test<br/>1 week, 10K users]
        C[Measure NDCG@10]
        D{NDCG improved?}
        E[Adjust Weights<br/>Grid Search]
        F[Final Weights]
        
        A --> B
        B --> C
        C --> D
        D -->|No| E
        E --> B
        D -->|Yes| F
    end
```

**ØªÙˆØ¶ÛŒØ­ Ù‡Ø± Weight:**


1. **BM25 (30%)**: Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† weight Ú†ÙˆÙ†:
   * Users Ø§ØºÙ„Ø¨ exact keywords Ù…ÛŒâ€ŒØ²Ù†Ù†Ø¯
   * Ù…Ø«Ù„Ø§Ù‹: "#sunset" â†’ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù†Ø¯ posts Ø¨Ø§ hashtag sunset Ø¨Ø¨ÛŒÙ†Ù†Ø¯
2. **Vector (25%)**: Ú©Ù…ÛŒ Ú©Ù…ØªØ± Ú†ÙˆÙ†:
   * Semantic search Ø®ÙˆØ¨ Ø§Ø³Øª ÙˆÙ„ÛŒ Ú¯Ø§Ù‡ÛŒ noisy Ø§Ø³Øª
   * Ù…Ù…Ú©Ù† Ø§Ø³Øª results irrelevant Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯
3. **Engagement (20%)**: Ù…Ù‡Ù… Ú†ÙˆÙ†:
   * Posts Ø¨Ø§ engagement Ø¨Ø§Ù„Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ quality Ø¯Ø§Ø±Ù†Ø¯
   * Users like Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ content popular Ø¨Ø¨ÛŒÙ†Ù†Ø¯
4. **Recency (15%)**: Ù…ØªÙˆØ³Ø· Ú†ÙˆÙ†:
   * Ø¨Ø±Ø§ÛŒ trending topics Ø®ÛŒÙ„ÛŒ Ù…Ù‡Ù… Ø§Ø³Øª
   * ÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ evergreen content Ú©Ù…ØªØ± Ù…Ù‡Ù…
5. **Authority (10%)**: Ú©Ù…ØªØ±ÛŒÙ† Ú†ÙˆÙ†:
   * Account authority Ú©Ù…ØªØ± Ø§Ø² content relevance Ù…Ù‡Ù… Ø§Ø³Øª
   * ÙˆÙ„ÛŒ verified accounts Ú©Ù…ÛŒ boost Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯

**Implementation Ø¯Ø± Elasticsearch:**

```json
{
  "query": {
    "script_score": {
      "query": {
        "bool": {
          "should": [
            {
              "multi_match": {
                "query": "{{query}}",
                "fields": ["caption^5", "hashtags^3"],
                "boost": 0.30
              }
            },
            {
              "script_score": {
                "query": {"match_all": {}},
                "script": {
                  "source": "cosineSimilarity(params.query_vector, 'caption_vector') + 1.0",
                  "params": {"query_vector": [...]},
                  "boost": 0.25
                }
              }
            }
          ]
        }
      },
      "script": {
        "source": """
          double baseScore = _score;
          
          // Engagement Score (20%)
          double engagement = doc['likes_count'].value + 
                            (doc['comments_count'].value * 2);
          double engScore = engagement / (engagement + 500);
          
          // Recency Score (15%) - exponential decay
          long ageMillis = new Date().getTime() - 
                          doc['created_at'].value.toInstant().toEpochMilli();
          double ageDays = ageMillis / (1000.0 * 60 * 60 * 24);
          double recScore = Math.pow(0.5, ageDays / 7.0);
          
          // Authority Score (10%)
          double authScore = doc['author_verified'].value ? 0.7 : 0.3;
          
          return baseScore + (engScore * 0.20) + (recScore * 0.15) + (authScore * 0.10);
        """
      }
    }
  }
}
```

**ØªÙˆØ¶ÛŒØ­ Script:**


1. **baseScore**: ØªØ±Ú©ÛŒØ¨ BM25 + Vector (Ø§Ø² should clauses)
2. **engScore**: Normalize engagement Ø¨Ø§ saturation function
   * Ú†Ø±Ø§ `/ (engagement + 500)`? ØªØ§ engagement Ø®ÛŒÙ„ÛŒ Ø¨Ø§Ù„Ø§ØŒ score Ø±Ø§ dominate Ù†Ú©Ù†Ø¯
3. **recScore**: Exponential decay Ø¨Ø§ half-life 7 Ø±ÙˆØ²
   * Ú†Ø±Ø§ 7 Ø±ÙˆØ²ØŸ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ content Ø¨Ø¹Ø¯ Ø§Ø² 1 Ù‡ÙØªÙ‡ Ú©Ù…ØªØ± relevant Ù…ÛŒâ€ŒØ´ÙˆØ¯
4. **authScore**: Simple binary (verified ÛŒØ§ Ù†Ù‡)

**Engagement Score Saturation:**

```python
import matplotlib.pyplot as plt

engagement = np.arange(0, 10000, 100)
score = engagement / (engagement + 500)

plt.plot(engagement, score)
plt.xlabel('Engagement (likes + comments)')
plt.ylabel('Normalized Score')
plt.title('Engagement Score Saturation')
plt.grid(True)
```

**Recency Decay:**

```python
days = np.arange(0, 30, 1)
recency_score = 0.5 ** (days / 7)

plt.plot(days, recency_score)
plt.xlabel('Age (days)')
plt.ylabel('Recency Score')
plt.title('Exponential Decay (Half-life = 7 days)')
plt.grid(True)
```

### 3.4 Learning-to-Rank (LTR)

**Ú†Ø±Ø§ LTRØŸ**

BM25 + Vector + hand-tuned weights Ø®ÙˆØ¨ Ø§Ø³ØªØŒ ÙˆÙ„ÛŒ:

* Optimal weights Ú†ÛŒØ³ØªØŸ (Ø´Ø§ÛŒØ¯ 0.30/0.25 Ø¨Ù‡ØªØ±ÛŒÙ† Ù†Ø¨Ø§Ø´Ø¯)
* Feature interactions Ú†Ù‡ Ù‡Ø³ØªÙ†Ø¯? (Ø´Ø§ÛŒØ¯ engagement Ã— recency ØªØ£Ø«ÛŒØ± Ø¯Ø§Ø±Ø¯)
* Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø² user behavior ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±ÛŒÙ…ØŸ

LTR Ø§ÛŒÙ† Ø³ÙˆØ§Ù„Ø§Øª Ø±Ø§ Ø¬ÙˆØ§Ø¨ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø¨Ø§ **Machine Learning**.

**LTR Ú†ÛŒØ³ØªØŸ**

LTR ÛŒÚ© supervised learning problem Ø§Ø³Øª:

```
Input: Query + List of Documents
Output: Ranked list of Documents
```

**Types of LTR:**

```mermaid
graph TB
    LTR[Learning-to-Rank]
    
    LTR --> PW[Pointwise]
    LTR --> PA[Pairwise]
    LTR --> LW[Listwise]
    
    PW --> PW1[Regression<br/>RMSE]
    PA --> PA1[RankNet<br/>LambdaRank]
    LW --> LW1[LambdaMART<br/>ListNet]
    
    style LW1 fill:#bfb,stroke:#333,stroke-width:2px
```

**Ú†Ø±Ø§ LambdaMARTØŸ**

| Approach | Pros | Cons |
|----|----|----|
| **Pointwise** | Simple | Ignores document order |
| **Pairwise** | Considers pairs | Doesn't optimize for metrics |
| **Listwise** (LambdaMART) | Optimizes NDCG | Complex |

**LambdaMART Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ**


1. **MART (Multiple Additive Regression Trees)**:
   * Ensemble of decision trees
   * Ù‡Ø± tree Ø®Ø·Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ correct Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (boosting)
2. **Lambda**:
   * Gradient Ø¨Ø±Ø§ÛŒ ranking metrics (NDCG)
   * Ù‡Ø± pair of documents ÛŒÚ© lambda Ø¯Ø§Ø±Ø¯

**Feature Engineering:**

```python
# Example features for a query-document pair
features = {
    # Text relevance
    'bm25_score': 2.34,
    'vector_similarity': 0.87,
    'query_term_coverage': 0.75,  # % of query terms matched
    'exact_match': 1,  # Binary: exact phrase found?
    
    # Engagement
    'likes_count': 1234,
    'comments_count': 56,
    'saves_count': 78,
    'engagement_rate': 0.042,
    
    # Temporal
    'recency_score': 0.65,
    'post_age_hours': 48,
    'is_trending': 0,
    
    # Author
    'author_followers': 50000,
    'author_engagement_rate': 0.035,
    'author_verified': 1,
    
    # Content
    'has_hashtags': 1,
    'hashtag_count': 3,
    'has_location': 1,
    'post_type': 'image',  # categorical
    'caption_length': 150
}
```

**Training Process:**

```mermaidjs
sequenceDiagram
    participant Logs as Click Logs
    participant Prep as Data Prep
    participant Train as Training
    participant Model as LTR Model
    participant Deploy as Deployment
    
    Logs->>Prep: Collect queries + clicks
    Note over Prep: Extract features<br/>Label relevance
    Prep->>Train: Training data
    Note over Train: LambdaMART<br/>Optimize NDCG
    Train->>Model: Trained model
    Model->>Deploy: Export model
    Note over Deploy: C# inference<br/>Re-rank top 100
```

**Relevance Labels:**

Ø§Ø² click data Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… relevance labels Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒÙ…:

| User Action | Relevance Label |
|----|----|
| No click | 0 (Irrelevant) |
| Click | 1 (Relevant) |
| Click + Like | 2 (Very Relevant) |
| Click + Save | 3 (Highly Relevant) |
| Click + Share | 4 (Extremely Relevant) |

**Training Code (Python/LightGBM):**

```python
import lightgbm as lgb
import pandas as pd
from sklearn.model_selection import train_test_split

# Load data
data = pd.read_csv('training_data.csv')

# Features
features = [
    'bm25_score', 'vector_similarity', 'query_term_coverage',
    'likes_count', 'comments_count', 'engagement_rate',
    'recency_score', 'author_followers', 'author_verified',
    'has_hashtags', 'has_location'
]

X = data[features]
y = data['relevance_label']  # 0-4
query_ids = data['query_id']

# Group data by query
group_sizes = query_ids.value_counts().sort_index().values

# Create LightGBM dataset
train_data = lgb.Dataset(X, label=y, group=group_sizes)

# LambdaMART parameters
params = {
    'objective': 'lambdarank',
    'metric': 'ndcg',
    'ndcg_eval_at': [1, 3, 5, 10],
    'learning_rate': 0.05,
    'num_leaves': 31,
    'min_data_in_leaf': 20,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'max_depth': 6
}

# Train
model = lgb.train(
    params,
    train_data,
    num_boost_round=100,
    valid_sets=[train_data],
    callbacks=[lgb.early_stopping(stopping_rounds=10)]
)

# Feature importance
importance = model.feature_importance(importance_type='gain')
feature_names = model.feature_name()

for name, imp in zip(feature_names, importance):
    print(f"{name}: {imp}")

# Export
model.save_model('ltr_model.txt')
```

**Feature Importance Example Output:**

```
vector_similarity: 2845.3
bm25_score: 2341.7
engagement_rate: 1876.4
recency_score: 1234.2
author_followers: 987.1
...
```

**Inference Ø¯Ø± C#:**

Ú†Ú¯ÙˆÙ†Ù‡ Ø§ÛŒÙ† model Ø±Ø§ Ø¯Ø± C# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…ØŸ

```csharp
// Option 1: ML.NET (recommended)
using Microsoft.ML;
using Microsoft.ML.Data;

public class LTRPredictor
{
    private readonly PredictionEngine<RankingFeatures, RankingPrediction> _engine;
    
    public LTRPredictor(string modelPath)
    {
        var mlContext = new MLContext();
        var model = mlContext.Model.Load(modelPath, out var schema);
        _engine = mlContext.Model.CreatePredictionEngine<RankingFeatures, RankingPrediction>(model);
    }
    
    public float Predict(RankingFeatures features)
    {
        var prediction = _engine.Predict(features);
        return prediction.Score;
    }
}

// Option 2: ONNX Runtime (faster)
using Microsoft.ML.OnnxRuntime;

public class ONNXPredictor
{
    private readonly InferenceSession _session;
    
    public ONNXPredictor(string modelPath)
    {
        _session = new InferenceSession(modelPath);
    }
    
    public float[] Predict(float[] features)
    {
        var inputTensor = new DenseTensor<float>(features, new[] { 1, features.Length });
        var inputs = new List<NamedOnnxValue>
        {
            NamedOnnxValue.CreateFromTensor("input", inputTensor)
        };
        
        using var results = _session.Run(inputs);
        var output = results.First().AsEnumerable<float>().ToArray();
        return output;
    }
}
```

**Re-ranking Pipeline:**

```csharp
public async Task<List<SearchResult>> SearchWithLTRAsync(string query)
{
    // Step 1: Retrieve candidates (top 100) with Elasticsearch
    var candidates = await _esClient.SearchAsync<Post>(s => s
        .Query(/* hybrid query */)
        .Size(100)
    );
    
    // Step 2: Extract features for each candidate
    var featuresList = candidates.Documents.Select(doc => 
        ExtractFeatures(query, doc)
    ).ToList();
    
    // Step 3: Re-rank with LTR model
    var scores = new List<(Post doc, float score)>();
    foreach (var (doc, features) in candidates.Documents.Zip(featuresList))
    {
        var score = _ltrPredictor.Predict(features);
        scores.Add((doc, score));
    }
    
    // Step 4: Sort by LTR score
    var reranked = scores.OrderByDescending(x => x.score)
                         .Take(20)
                         .Select(x => x.doc)
                         .ToList();
    
    return reranked;
}
```

**Performance Consideration:**

* LTR inference Ø±ÙˆÛŒ 100 documents Ø³Ø±ÛŒØ¹ Ø§Ø³Øª (\~10-20ms)
* ÙˆÙ„ÛŒ Ø±ÙˆÛŒ 10,000 documents slow Ù…ÛŒâ€ŒØ´ÙˆØ¯
* Ù¾Ø³ Ù…Ø§ ÙÙ‚Ø· top 100 Ø§Ø² Elasticsearch Ø±Ø§ re-rank Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…

**Continuous Learning:**

```mermaidjs
graph LR
    A[User Interactions] --> B[Click Logs]
    B --> C[Feature Extraction]
    C --> D[Daily Training]
    D --> E[Model Evaluation]
    E --> F{NDCG improved?}
    F -->|Yes| G[Deploy New Model]
    F -->|No| H[Keep Old Model]
    G --> I[Production]
    H --> I
    I --> A
```


---

## 4. Data Pipeline - ØªÙˆØ¶ÛŒØ­ Ú©Ø§Ù…Ù„

### 4.1 Ú†Ø±Ø§ Lambda ArchitectureØŸ

```mermaidjs
graph TB
    subgraph "Lambda Architecture"
        DS[Data Source]
        
        subgraph "Speed Layer (Real-time)"
            K[Kafka]
            F[Flink]
            ES[Elasticsearch]
        end
        
        subgraph "Batch Layer (Offline)"
            SQL[SQL Server]
            BATCH[Batch Jobs]
        end
        
        subgraph "Serving Layer"
            API[Search API]
        end
        
        DS --> K
        K --> F
        F --> ES
        F --> SQL
        
        SQL --> BATCH
        BATCH --> ES
        
        ES --> API
        SQL --> API
    end
```

**ØªÙˆØ¶ÛŒØ­:**


1. **Speed Layer**: Ø¨Ø±Ø§ÛŒ real-time data (new posts)
   * Kafka â†’ Flink â†’ Elasticsearch
   * Latency: <30 seconds
2. **Batch Layer**: Ø¨Ø±Ø§ÛŒ historical data Ùˆ corrections
   * SQL Server â†’ Batch Jobs â†’ Elasticsearch
   * Frequency: Ø±ÙˆØ²Ø§Ù†Ù‡ ÛŒØ§ Ù‡ÙØªÚ¯ÛŒ
3. **Serving Layer**: ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ Ø§Ø² Ù‡Ø± Ø¯Ùˆ

**Ú†Ø±Ø§ Ø§ÛŒÙ† approachØŸ**

* **Freshness**: New content Ø³Ø±ÛŒØ¹ indexable Ù…ÛŒâ€ŒØ´ÙˆØ¯
* **Accuracy**: Historical data Ø¨Ø§ batch jobs ØµØ­ÛŒØ­ Ù…ÛŒâ€ŒØ´ÙˆØ¯
* **Resilience**: Ø§Ú¯Ø± streaming fail Ø´ÙˆØ¯ØŒ batch Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ compensate Ú©Ù†Ø¯

### 4.2 Kafka Topic Design

**Ú†Ø±Ø§ Ú†Ù†Ø¯ TopicØŸ**

```
raw-posts          â†’ unprocessed data
processed-posts    â†’ after validation/normalization
enrichment-queue   â†’ for async enrichment (OCR, transcription)
dead-letter-queue  â†’ failed messages
```

**Partitioning Strategy:**

```python
# Partition by user_id (to maintain order per user)
partition = hash(user_id) % num_partitions
```

**Ú†Ø±Ø§ØŸ**

* Ù‡Ù…Ù‡ posts Ø§Ø² ÛŒÚ© user Ø¨Ù‡ ÛŒÚ© partition Ù…ÛŒâ€ŒØ±ÙˆÙ†Ø¯
* ØªØ±ØªÛŒØ¨ posts per user Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯
* Ù‡Ø± consumer ÛŒÚ© subset Ø§Ø² users Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯

**Retention Policy:**

```yaml
raw-posts:
  retention: 7 days
  reason: "Ø¨Ø±Ø§ÛŒ replay Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ú©Ù„"

processed-posts:
  retention: 7 days
  reason: "Ø¨Ø±Ø§ÛŒ downstream consumers"

dead-letter-queue:
  retention: 30 days
  reason: "Ø¨Ø±Ø§ÛŒ debugging"
```

### 4.3 Flink Stream Processing

**Ú†Ø±Ø§ Flink ØŸ**

```mermaidjs
graph LR
    A[Kafka Source] --> B[Flink Operators]
    B --> C[Kafka Sink]
    B --> D[JDBC Sink]
    
    subgraph "Flink Features"
        B --> E[Windowing]
        B --> F[State Management]
        B --> G[Exactly-Once]
    end
```

**Ù…Ø«Ø§Ù„: Deduplication Window**

```java
DataStream<Post> stream = env
    .addSource(new FlinkKafkaConsumer<>("raw-posts", deserializer, props))
    .keyBy(post -> post.getPostId())
    .window(TumblingEventTimeWindows.of(Time.hours(24)))
    .reduce(new DeduplicationFunction());

class DeduplicationFunction implements ReduceFunction<Post> {
    @Override
    public Post reduce(Post p1, Post p2) {
        // Keep the most recent
        return p1.getTimestamp() > p2.getTimestamp() ? p1 : p2;
    }
}
```

**ØªÙˆØ¶ÛŒØ­:**


1. **keyBy(postId)**: Ù‡Ù…Ù‡ events Ø¨Ø§ ÛŒÚ© post_id Ø¨Ù‡ ÛŒÚ© operator Ù…ÛŒâ€ŒØ±ÙˆÙ†Ø¯
2. **window(24h)**: ÛŒÚ© window 24 Ø³Ø§Ø¹ØªÙ‡
3. **reduce**: Ø§Ú¯Ø± duplicate posts Ø¯Ø± window Ø¨Ø§Ø´Ù†Ø¯ØŒ ÙÙ‚Ø· Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…

**Ú†Ø±Ø§ 24 Ø³Ø§Ø¹ØªØŸ**

* Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ duplicates Ø¯Ø± Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ Ø§ÙˆÙ„ Ù…ÛŒâ€ŒØ¢ÛŒÙ†Ø¯
* 24 Ø³Ø§Ø¹Øª Ú©Ø§ÙÛŒ Ø§Ø³Øª ØªØ§ Ù‡Ù…Ù‡ duplicates Ø±Ø§ Ø¨Ú¯ÛŒØ±ÛŒÙ…
* Ø¨ÛŒØ´ØªØ± Ø§Ø² Ø§ÛŒÙ† state overhead Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ±ÙˆØ¯

**Exactly-Once Semantics:**

```java
// Enable checkpointing
env.enableCheckpointing(60000); // 60 seconds
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// Kafka sink with transactions
FlinkKafkaProducer<Post> sink = new FlinkKafkaProducer<>(
    "processed-posts",
    new PostSerializer(),
    props,
    FlinkKafkaProducer.Semantic.EXACTLY_ONCE
);
```

**Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ**


1. Flink Ù‡Ø± 60 Ø«Ø§Ù†ÛŒÙ‡ checkpoint Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ (snapshot of state)
2. Ø§Ú¯Ø± failure Ø´ÙˆØ¯ØŒ Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† checkpoint restart Ù…ÛŒâ€ŒÚ©Ù†Ø¯
3. Kafka transactions ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ messages exactly once Ù†ÙˆØ´ØªÙ‡ Ø´ÙˆÙ†Ø¯

### 4.4 Data Normalization

**Ú†Ø±Ø§ Normalization Ù…Ù‡Ù… Ø§Ø³ØªØŸ**

Social media data Ø¨Ø³ÛŒØ§Ø± messy Ø§Ø³Øª:

```json
{
  "caption": "  Check out this    AMAZING ğŸ”¥ğŸ”¥ sunset!!!  #sunset  #beach   ",
  "created_at": "2024-11-15 10:30:00",
  "hashtags": ["#sunset", "#Sunset", "#SUNSET", "#beach"]
}
```

Ù…Ø´Ú©Ù„Ø§Øª:

* Extra whitespace
* Mixed case
* Duplicate hashtags (case-insensitive)
* Emojis

**Normalization Pipeline:**

```csharp
public class TextNormalizer
{
    public string Normalize(string text)
    {
        if (string.IsNullOrWhiteSpace(text))
            return string.Empty;
        
        // 1. Remove HTML tags
        text = Regex.Replace(text, @"<[^>]+>", string.Empty);
        
        // 2. Normalize Unicode (NFD â†’ NFC)
        text = text.Normalize(NormalizationForm.FormC);
        
        // 3. Handle emojis (keep them, but normalize variants)
        text = NormalizeEmojis(text);
        
        // 4. Remove excessive whitespace
        text = Regex.Replace(text, @"\s+", " ");
        
        // 5. Trim
        return text.Trim();
    }
    
    private string NormalizeEmojis(string text)
    {
        // Some emojis have variants (e.g., ğŸ”¥ï¸ vs ğŸ”¥)
        // Normalize to standard form
        return text; // Simplified
    }
}

public class HashtagNormalizer
{
    public List<string> Normalize(List<string> hashtags)
    {
        return hashtags
            .Select(h => h.TrimStart('#').ToLowerInvariant())
            .Distinct()
            .ToList();
    }
}
```

**Timestamp Normalization:**

```csharp
public class TimestampNormalizer
{
    public DateTime Normalize(string timestamp)
    {
        // Parse various formats
        DateTime dt;
        
        if (DateTime.TryParse(timestamp, out dt))
        {
            // Convert to UTC
            if (dt.Kind == DateTimeKind.Local)
                dt = dt.ToUniversalTime();
            else if (dt.Kind == DateTimeKind.Unspecified)
                dt = DateTime.SpecifyKind(dt, DateTimeKind.Utc);
            
            return dt;
        }
        
        throw new ArgumentException($"Invalid timestamp: {timestamp}");
    }
}
```

**Ú†Ø±Ø§ UTCØŸ**

* Timezone-independent
* Elasticsearch Ø¨Ù‡ UTC expect Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* Comparison Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø§Ø³Øª

### 4.5 Embedding Generation Pipeline

```mermaidjs
graph TB
    subgraph "Embedding Pipeline"
        P[New Post] --> Q[Embedding Queue]
        Q --> W[Worker Pool]
        W --> M[ML Model]
        M --> V[Vector]
        V --> SQL[(SQL Server)]
        V --> ES[(Elasticsearch)]
    end
    
    subgraph "Worker Pool"
        W --> W1[Worker 1<br/>GPU]
        W --> W2[Worker 2<br/>GPU]
        W --> W3[Worker 3<br/>GPU]
    end
```

**Ú†Ø±Ø§ AsyncØŸ**

Embedding generation slow Ø§Ø³Øª (100-200ms per text). Ø§Ú¯Ø± synchronous Ø¨Ø§Ø´Ø¯:

* User Ø¨Ø§ÛŒØ¯ 200ms Ù…Ù†ØªØ¸Ø± Ø¨Ù…Ø§Ù†Ø¯
* Throughput Ù¾Ø§ÛŒÛŒÙ† Ù…ÛŒâ€ŒØ´ÙˆØ¯ (5-10 TPS)

Ø¨Ø§ async approach:

* User ÙÙˆØ±Ø§Ù‹ response Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
* Embedding Ø¯Ø± background generate Ù…ÛŒâ€ŒØ´ÙˆØ¯
* Throughput Ø¨Ø§Ù„Ø§ (1000+ TPS)

**Worker Pool Implementation:**

```python
import asyncio
from sentence_transformers import SentenceTransformer
import torch

class EmbeddingWorker:
    def __init__(self, model_name, device='cuda'):
        self.model = SentenceTransformer(model_name, device=device)
        self.batch_size = 32
    
    async def process_batch(self, texts):
        # Generate embeddings in batch (efficient!)
        embeddings = self.model.encode(
            texts,
            batch_size=self.batch_size,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        return embeddings
    
    async def run(self, queue):
        while True:
            # Accumulate batch
            batch = []
            for _ in range(self.batch_size):
                try:
                    item = await asyncio.wait_for(queue.get(), timeout=1.0)
                    batch.append(item)
                except asyncio.TimeoutError:
                    break
            
            if not batch:
                continue
            
            # Process batch
            texts = [item['text'] for item in batch]
            embeddings = await self.process_batch(texts)
            
            # Save to DB
            for item, embedding in zip(batch, embeddings):
                await self.save_embedding(item['post_id'], embedding)

# Run multiple workers
async def main():
    queue = asyncio.Queue()
    
    workers = [
        EmbeddingWorker('all-MiniLM-L12-v2', device=f'cuda:{i}')
        for i in range(3)  # 3 GPUs
    ]
    
    tasks = [worker.run(queue) for worker in workers]
    await asyncio.gather(*tasks)
```

**Batch Processing Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ**

```python
# Single processing
for text in texts:
    embedding = model.encode(text)  # 100ms per text
# Total: 100ms Ã— 100 = 10 seconds

# Batch processing
embeddings = model.encode(texts, batch_size=32)  # 500ms for 32 texts
# Total: 500ms Ã— 4 = 2 seconds (5Ã— faster!)
```

GPU Ù‡Ø§ Ø¨Ø±Ø§ÛŒ parallel processing optimized Ù‡Ø³ØªÙ†Ø¯.


---

## 5. UI/UX Design - ØªÙˆØ¶ÛŒØ­ Ú©Ø§Ù…Ù„

### 5.1 Ú†Ø±Ø§ Simple + Advanced Search?

```mermaidjs
graph TB
    U[User] --> D{Expertise?}
    D -->|Casual| S[Simple Search]
    D -->|Professional| A[Advanced Search]
    
    S --> S1[Single input box]
    S --> S2[Autocomplete]
    S --> S3[Smart defaults]
    
    A --> A1[Multiple filters]
    A --> A2[Boolean operators]
    A --> A3[Export options]
```

**User Personas:**

| Persona | Need | Interface |
|----|----|----|
| Casual User | "Find sunset photos" | Simple search |
| Marketer | "Posts with >10K likes, last 7 days, location=NYC" | Advanced search |
| Analyst | "Export all posts with #BlackFriday" | Advanced + Export |

### 5.2 Autocomplete Algorithm

**Ú†Ø±Ø§ AutocompleteØŸ**

* 50% of queries Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ autocompleted Ø´ÙˆÙ†Ø¯
* Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø±Ø§Ø­Øªâ€ŒØªØ± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ search Ú©Ù†Ù†Ø¯
* Spelling mistakes Ú©Ù…ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Implementation:**

```json
{
  "suggest": {
    "user-suggest": {
      "prefix": "john",
      "completion": {
        "field": "username.suggest",
        "size": 5,
        "skip_duplicates": true,
        "fuzzy": {
          "fuzziness": 1
        }
      }
    }
  }
}
```

**Fuzziness Ú†ÛŒØ³ØªØŸ**

```
User types: "johm"
Fuzzy matches:
  - john (edit distance = 1)
  - johny (edit distance = 1)
```

Edit distance = ØªØ¹Ø¯Ø§Ø¯ operations (insert/delete/replace) Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ ÛŒÚ© string Ø¨Ù‡ Ø¯ÛŒÚ¯Ø±ÛŒ.

**Ú†Ø±Ø§ Completion Suggester Ù†Ù‡ Search-as-you-typeØŸ**

| Feature | Completion | Search-as-you-type |
|----|----|----|
| **Speed** | Very fast (<10ms) | Slower (\~50ms) |
| **Index size** | Small | Large |
| **Flexibility** | Prefix only | Infix match |
| **Use case** | Autocomplete | Full search |

Ù…Ø§ Completion Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú†ÙˆÙ†:

* Ø³Ø±Ø¹Øª critical Ø§Ø³Øª (user Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ø¯)
* Prefix match Ú©Ø§ÙÛŒ Ø§Ø³Øª (user Ø§Ø² Ø§ÙˆÙ„ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ø¯)

### 5.3 Result Ranking UI

**Ú†Ø±Ø§ Diversity Ù…Ù‡Ù… Ø§Ø³ØªØŸ**

Ø§Ú¯Ø± top 10 results Ù‡Ù…Ù‡ Ø§Ø² ÛŒÚ© user Ø¨Ø§Ø´Ù†Ø¯ â†’ bad UX

**Diversity Algorithm:**

```python
def diversify_results(results, max_per_user=2):
    diverse = []
    user_counts = {}
    
    for result in results:
        user = result.username
        count = user_counts.get(user, 0)
        
        if count < max_per_user:
            diverse.append(result)
            user_counts[user] = count + 1
    
    return diverse
```

**Visual Indicators:**

```jsx
const PostCard = ({ post, queryTerms }) => {
    return (
        <div className="post-card">
            {/* Relevance indicator */}
            {post.score > 0.9 && (
                <Badge color="green">Highly Relevant</Badge>
            )}
            
            {/* Highlight matched terms */}
            <Caption 
                text={post.caption}
                highlight={queryTerms}
            />
            
            {/* Social proof */}
            <Engagement>
                <Icon name="like" /> {formatNumber(post.likes)}
                <Icon name="comment" /> {formatNumber(post.comments)}
            </Engagement>
            
            {/* Freshness */}
            {post.ageHours < 24 && (
                <Badge color="blue">New</Badge>
            )}
        </div>
    );
};
```

### 5.4 Accessibility (A11y)

**Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ**

* 15% of population Ø¯Ø§Ø±Ø§ÛŒ disability Ù‡Ø³ØªÙ†Ø¯
* Screen readers Ø¨Ø±Ø§ÛŒ blind users
* Keyboard navigation Ø¨Ø±Ø§ÛŒ motor disabilities
* Legal requirements (ADA compliance)

**ARIA Labels:**

```jsx
<input
    type="text"
    role="searchbox"
    aria-label="Search Instagram posts and pages"
    aria-describedby="search-help"
    aria-autocomplete="list"
    aria-controls="suggestions-list"
    aria-expanded={suggestions.length > 0}
/>

<div id="search-help" className="sr-only">
    Enter keywords to search for posts and pages
</div>
```

**Keyboard Navigation:**

```jsx
const SearchBar = () => {
    const handleKeyDown = (e) => {
        switch(e.key) {
            case 'ArrowDown':
                // Move to next suggestion
                setSelectedIndex(prev => prev + 1);
                break;
            case 'ArrowUp':
                // Move to previous suggestion
                setSelectedIndex(prev => prev - 1);
                break;
            case 'Enter':
                // Select current suggestion
                selectSuggestion(suggestions[selectedIndex]);
                break;
            case 'Escape':
                // Close suggestions
                setSuggestions([]);
                break;
        }
    };
    
    return <input onKeyDown={handleKeyDown} />;
};
```

**Color Contrast:**

```css
/* âŒ Bad: low contrast */
.button {
    background: #ddd;
    color: #aaa;
}

/* âœ… Good: WCAG AAA compliant */
.button {
    background: #0066cc;
    color: #ffffff;
}
```

WCAG (Web Content Accessibility Guidelines) standard:

* AA: contrast ratio â‰¥ 4.5:1
* AAA: contrast ratio â‰¥ 7:1


---

## 6. Performance Optimization - ØªÙˆØ¶ÛŒØ­ Ø¹Ù…ÛŒÙ‚

### 6.1 Caching Strategy - Multi-Layer

```mermaidjs
graph TB
    Q[Query] --> L1{Browser Cache}
    L1 -->|Hit| R1[Return cached]
    L1 -->|Miss| L2{Redis Cache}
    L2 -->|Hit| R2[Return cached]
    L2 -->|Miss| L3{ES Query Cache}
    L3 -->|Hit| R3[Return cached]
    L3 -->|Miss| ES[Elasticsearch]
    ES --> STORE[Store in all caches]
    STORE --> R4[Return result]
```

**Ú†Ø±Ø§ Multi-LayerØŸ**

| Layer | Hit Rate | Latency | Coverage |
|----|----|----|----|
| Browser | 20% | <1ms | Same user, same query |
| Redis | 50% | \~5ms | All users, popular queries |
| ES Query | 70% | \~20ms | Recent queries |

Combined hit rate:

```
P(hit) = 0.20 + (0.80 Ã— 0.50) + (0.80 Ã— 0.50 Ã— 0.70)
       = 0.20 + 0.40 + 0.28
       = 88% hit rate
```

**Cache Invalidation:**

"There are only two hard problems in Computer Science: cache invalidation and naming things."

**Strategies:**


1. **TTL (Time To Live)**:

   ```
   Browser: 5 minutes
   Redis: 10 minutes
   ES: 1 hour
   ```
2. **Event-Based Invalidation**:

   ```csharp
   public async Task OnPostUpdated(Post post)
   {
       // Invalidate caches that might contain this post
       var patterns = new[]
       {
           $"search:*{post.Username}*",
           $"search:*{string.Join("*", post.Hashtags)}*"
       };
       
       foreach (var pattern in patterns)
       {
           await _cache.InvalidatePatternAsync(pattern);
       }
   }
   ```

**Cache Stampede Problem:**

```mermaidjs
sequenceDiagram
    participant C1 as Client 1
    participant C2 as Client 2
    participant C3 as Client 3
    participant Cache
    participant DB
    
    C1->>Cache: GET key
    Cache-->>C1: MISS
    C2->>Cache: GET key
    Cache-->>C2: MISS
    C3->>Cache: GET key
    Cache-->>C3: MISS
    
    par All hit DB
        C1->>DB: Query
        C2->>DB: Query
        C3->>DB: Query
    end
    
    Note over DB: Overload!
```

**Solution: Lock Pattern**

```csharp
public async Task<SearchResult> GetOrComputeAsync(string cacheKey)
{
    // Try cache
    var cached = await _cache.GetAsync(cacheKey);
    if (cached != null)
        return cached;
    
    // Acquire lock
    var lockKey = $"lock:{cacheKey}";
    var lockAcquired = await _cache.LockAsync(lockKey, TimeSpan.FromSeconds(10));
    
    if (lockAcquired)
    {
        try
        {
            // Double-check cache (maybe another thread filled it)
            cached = await _cache.GetAsync(cacheKey);
            if (cached != null)
                return cached;
            
            // Compute
            var result = await ComputeExpensiveOperationAsync();
            
            // Store
            await _cache.SetAsync(cacheKey, result, TimeSpan.FromMinutes(10));
            
            return result;
        }
        finally
        {
            await _cache.UnlockAsync(lockKey);
        }
    }
    else
    {
        // Wait for other thread to finish
        await Task.Delay(100);
        return await GetOrComputeAsync(cacheKey); // Retry
    }
}
```

### 6.2 Connection Pooling

**Ú†Ø±Ø§ PoolØŸ**

```
Without pooling:
  Request 1: Create connection (100ms) + Query (10ms) + Close (10ms) = 120ms
  Request 2: Create connection (100ms) + Query (10ms) + Close (10ms) = 120ms
  ...

With pooling:
  Request 1: Get from pool (1ms) + Query (10ms) + Return to pool (1ms) = 12ms
  Request 2: Get from pool (1ms) + Query (10ms) + Return to pool (1ms) = 12ms
  ...

10Ã— faster!
```

**Elasticsearch Connection Pool:**

```csharp
var pool = new StaticConnectionPool(new[]
{
    new Uri("https://es-node1:9200"),
    new Uri("https://es-node2:9200"),
    new Uri("https://es-node3:9200")
});

var settings = new ConnectionSettings(pool)
    .MaxRetries(3)
    .MaxRetryTimeout(TimeSpan.FromMinutes(5))
    .DeadTimeout(TimeSpan.FromSeconds(60)); // Mark node as dead after 60s
```

**Round-Robin Load Balancing:**

```
Request 1 â†’ Node 1
Request 2 â†’ Node 2
Request 3 â†’ Node 3
Request 4 â†’ Node 1
...
```

**Sniffing (Dynamic Node Discovery):**

```csharp
var pool = new SniffingConnectionPool(new[]
{
    new Uri("https://es-node1:9200")
});

var settings = new ConnectionSettings(pool)
    .EnableHttpCompression()
    .SniffLifeSpan(TimeSpan.FromMinutes(5)); // Re-discover nodes every 5 min
```

Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ


1. Ø§ØªØµØ§Ù„ Ø¨Ù‡ es-node1
2. Query: `GET /_nodes`
3. Ù¾Ø§Ø³Ø®: Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… nodes Ø¯Ø± cluster
4. Update pool Ø¨Ø§ ØªÙ…Ø§Ù… nodes

**SQL Server Connection Pool:**

```
ConnectionString:
"Server=sql-server;Database=InstagramData;
 Min Pool Size=10;
 Max Pool Size=100;
 Connection Lifetime=300;"
```

Parameters:

* **Min Pool Size**: ØªØ¹Ø¯Ø§Ø¯ connections Ú©Ù‡ Ù‡Ù…ÛŒØ´Ù‡ open Ù‡Ø³ØªÙ†Ø¯
* **Max Pool Size**: Ø­Ø¯Ø§Ú©Ø«Ø± connections Ù‡Ù…Ø²Ù…Ø§Ù†
* **Connection Lifetime**: Ø¨Ø¹Ø¯ Ø§Ø² 300 Ø«Ø§Ù†ÛŒÙ‡ØŒ connection Ø¨Ø³ØªÙ‡ Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø§Ø² Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø¨Ø±Ø§ÛŒ load balancing)

### 6.3 Bulk Operations

**Ú†Ø±Ø§ BulkØŸ**

```
Individual requests:
  POST /index/_doc/1 â†’ 50ms
  POST /index/_doc/2 â†’ 50ms
  ...
  POST /index/_doc/1000 â†’ 50ms
  Total: 50 seconds

Bulk request:
  POST /_bulk
  [1000 documents]
  Total: 500ms (100Ã— faster!)
```

**Bulk API:**

```json
POST /_bulk
{"index":{"_index":"instagram_posts","_id":"1"}}
{"post_id":"1","caption":"Beautiful sunset","likes":100}
{"index":{"_index":"instagram_posts","_id":"2"}}
{"post_id":"2","caption":"Amazing view","likes":50}
```

**Optimal Batch Size:**

```python
import time

def benchmark_batch_size():
    sizes = [100, 500, 1000, 2000, 5000]
    
    for size in sizes:
        start = time.time()
        bulk_index(documents, batch_size=size)
        elapsed = time.time() - start
        
        throughput = len(documents) / elapsed
        print(f"Size: {size}, Throughput: {throughput:.0f} docs/sec")
```

**Typical Output:**

```
Size: 100,  Throughput: 5,000 docs/sec
Size: 500,  Throughput: 15,000 docs/sec
Size: 1000, Throughput: 20,000 docs/sec
Size: 2000, Throughput: 18,000 docs/sec  â† Optimal
Size: 5000, Throughput: 10,000 docs/sec
```

Ú†Ø±Ø§ 2000 optimal Ø§Ø³ØªØŸ

* Ú©Ù…ØªØ±: Overhead of multiple requests
* Ø¨ÛŒØ´ØªØ±: Network packet too largeØŒ timeout risk

**Error Handling Ø¯Ø± Bulk:**

```csharp
var response = await _client.BulkAsync(bulkDescriptor);

if (response.Errors)
{
    foreach (var item in response.ItemsWithErrors)
    {
        _logger.LogError(
            "Failed to index document {Id}: {Error}",
            item.Id,
            item.Error.Reason
        );
        
        // Send to DLQ
        await _dlq.SendAsync(new
        {
            DocumentId = item.Id,
            Error = item.Error.Reason,
            Timestamp = DateTime.UtcNow
        });
    }
}
```

### 6.4 Query Optimization

**Filter Context vs Query Context:**

```json
{
  "query": {
    "bool": {
      "must": [
        // Query context: scoring enabled
        {"match": {"caption": "sunset"}}
      ],
      "filter": [
        // Filter context: no scoring, cacheable
        {"term": {"post_type": "image"}},
        {"range": {"created_at": {"gte": "2024-01-01"}}}
      ]
    }
  }
}
```

**Ú†Ø±Ø§ Ø§ÛŒÙ† ØªÙØ§ÙˆØª Ù…Ù‡Ù… Ø§Ø³ØªØŸ**

| Aspect | Query Context | Filter Context |
|----|----|----|
| **Scoring** | Yes | No |
| **Caching** | No | Yes |
| **Use case** | Relevance | Filtering |

**Ù…Ø«Ø§Ù„:**

```
Query: "sunset beach"
Filters: type=image, date>2024-01-01

Query context (slow):
  {
    "bool": {
      "must": [
        {"match": {"caption": "sunset beach"}},
        {"term": {"post_type": "image"}},
        {"range": {"created_at": {"gte": "2024-01-01"}}}
      ]
    }
  }

Filter context (fast):
  {
    "bool": {
      "must": [
        {"match": {"caption": "sunset beach"}}
      ],
      "filter": [
        {"term": {"post_type": "image"}},
        {"range": {"created_at": {"gte": "2024-01-01"}}}
      ]
    }
  }
```

**Profile API (Query Performance Analysis):**

```json
GET /instagram_posts/_search
{
  "profile": true,
  "query": { ... }
}
```

**Output:**

```json
{
  "profile": {
    "shards": [
      {
        "searches": [
          {
            "query": [
              {
                "type": "TermQuery",
                "description": "post_type:image",
                "time_in_nanos": 5234567,
                "breakdown": {
                  "score": 0,
                  "build_scorer": 1234567,
                  "match": 4000000
                }
              }
            ]
          }
        ]
      }
    ]
  }
}
```

Ø§ÛŒÙ† Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯:

* `TermQuery` Ø±ÙˆÛŒ `post_type` 5.2ms Ø·ÙˆÙ„ Ú©Ø´ÛŒØ¯
* `build_scorer`: 1.2ms
* `match`: 4.0ms

Ø§Ú¯Ø± ÛŒÚ© query Ø®ÛŒÙ„ÛŒ slow Ø§Ø³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… identify Ú©Ù†ÛŒÙ… Ú©Ø¯Ø§Ù… Ø¨Ø®Ø´ Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ø¯.


---

## 7. Monitoring & Observability - Ø¹Ù…ÛŒÙ‚

### 7.1 Ú†Ø±Ø§ Observability Ù…Ù‡Ù… Ø§Ø³ØªØŸ

```
"Hope is not a strategy."
```

Ø¨Ø¯ÙˆÙ† monitoring:

* Ù†Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒÙ… Ø³ÛŒØ³ØªÙ… Ú†Ú¯ÙˆÙ†Ù‡ perform Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* ÙˆÙ‚ØªÛŒ Ù…Ø´Ú©Ù„ Ù¾ÛŒØ´ Ù…ÛŒâ€ŒØ¢ÛŒØ¯ØŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… diagnose Ú©Ù†ÛŒÙ…
* Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… optimize Ú©Ù†ÛŒÙ…

**Three Pillars of Observability:**

```mermaidjs
graph TB
    O[Observability]
    
    O --> M[Metrics]
    O --> L[Logs]
    O --> T[Traces]
    
    M --> M1[Time-series data<br/>Quantitative]
    L --> L1[Events<br/>Qualitative]
    T --> T1[Request flow<br/>Distributed]
```

### 7.2 Metrics - ØªØ¹Ø±ÛŒÙ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡

**Types of Metrics:**


1. **Counter**: ÙÙ‚Ø· increase Ù…ÛŒâ€ŒØ´ÙˆØ¯

   ```
   search_requests_total
   search_errors_total
   ```
2. **Gauge**: Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ increase/decrease Ø´ÙˆØ¯

   ```
   active_connections
   memory_usage_bytes
   ```
3. **Histogram**: distribution of values

   ```
   search_latency_seconds
   ```

**Ú†Ø±Ø§ HistogramØŸ**

```python
# Bad: Average only
avg_latency = sum(latencies) / len(latencies)
print(f"Average: {avg_latency}ms")

# Output: 100ms

# But:
latencies = [10, 10, 10, ..., 10, 5000]  # One outlier!
```

Average Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ 100msØŒ ÙˆÙ„ÛŒ 1% requests 5 Ø«Ø§Ù†ÛŒÙ‡ Ø·ÙˆÙ„ Ù…ÛŒâ€ŒÚ©Ø´Ø¯!

**Histogram Ø¨Ù‡ØªØ± Ø§Ø³Øª:**

```
search_latency_seconds_bucket{le="0.1"}  9500
search_latency_seconds_bucket{le="0.5"}  9900
search_latency_seconds_bucket{le="1.0"}  9950
search_latency_seconds_bucket{le="5.0"}  10000
```

Ø§ÛŒÙ† Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯:

* 95% requests <100ms
* 99% requests <500ms
* 99.5% requests <1000ms
* 100% requests <5000ms

**Percentiles (P50, P95, P99):**

```
P50 (median): 50% requests faster, 50% slower
P95: 95% requests faster, 5% slower
P99: 99% requests faster, 1% slower
```

P99 Ù…Ù‡Ù… Ø§Ø³Øª Ú†ÙˆÙ† outliers Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.

**PromQL Queries:**

```promql
# P95 latency over last 5 minutes
histogram_quantile(0.95, 
  rate(search_latency_seconds_bucket[5m])
)

# Requests per second
rate(search_requests_total[1m])

# Error rate
rate(search_errors_total[5m]) 
  / rate(search_requests_total[5m])
```

### 7.3 Distributed Tracing

**Ú†Ø±Ø§ TracingØŸ**

```
User request â†’ API Gateway â†’ Search Service â†’ Elasticsearch

If slow (1000ms), where is the bottleneck?
- API Gateway? (50ms)
- Search Service? (200ms)
- Elasticsearch? (750ms)  â† Found it!
```

Tracing Ø§ÛŒÙ† Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

**OpenTelemetry:**

```csharp
using var activity = _activitySource.StartActivity("SearchAsync");

// Add attributes
activity?.SetTag("search.query", query);
activity?.SetTag("search.type", type);

// Child span
using var esActivity = _activitySource.StartActivity("ElasticsearchQuery");
var results = await _client.SearchAsync(...);
esActivity?.SetTag("es.hits", results.Total);
esActivity?.SetStatus(ActivityStatusCode.Ok);

// Parent span
activity?.SetStatus(ActivityStatusCode.Ok);
```

**Jaeger UI:**

```
Search Request (1000ms total)
â”‚
â”œâ”€ Parse Query (10ms)
â”œâ”€ Check Cache (50ms)
â””â”€ Elasticsearch Query (940ms)
   â”‚
   â”œâ”€ BM25 Search (400ms)
   â”œâ”€ Vector Search (500ms)
   â””â”€ Merge Results (40ms)
```

Ø§ÛŒÙ† visualization Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ Vector Search slow Ø§Ø³Øª.

**Sampling:**

```csharp
// Sample 10% of requests
var sampler = new TraceIdRatioBasedSampler(0.1);
```

Ú†Ø±Ø§ samplingØŸ

* Tracing overhead Ø¯Ø§Ø±Ø¯
* 100% tracing Ø®ÛŒÙ„ÛŒ data ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* 10% sampling Ú©Ø§ÙÛŒ Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ debugging

### 7.4 Alerting - Smart Alerts

**Ú†Ø±Ø§ Smart AlertsØŸ**

```
âŒ Bad alert:
"CPU usage > 80%"

Problem: CPU usage fluctuates, Ù…Ù…Ú©Ù† Ø§Ø³Øª false positive Ø¨Ø§Ø´Ø¯
```

```
âœ… Good alert:
"CPU usage > 80% for 5 minutes"

This filters noise!
```

**Alert Best Practices:**


1. **Actionable**: Ø¨Ø§ÛŒØ¯ Ø¨Ø¯Ø§Ù†ÛŒÙ… Ú†Ù‡ Ú©Ø§Ø± Ú©Ù†ÛŒÙ…
2. **Relevant**: Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ user impact Ø¨Ø§Ø´Ø¯
3. **Threshold tuning**: Ø¨Ø± Ø§Ø³Ø§Ø³ historical data

**Example Alert Rule:**

```yaml
- alert: SearchLatencyHigh
  expr: |
    histogram_quantile(0.95, 
      rate(search_latency_seconds_bucket[5m])
    ) > 0.5
  for: 5m
  labels:
    severity: critical
    team: search
  annotations:
    summary: "Search P95 latency above 500ms"
    description: "Current P95: {{ $value }}s"
    runbook: "https://wiki.company.com/runbooks/search-latency"
```

**ØªÙˆØ¶ÛŒØ­:**

* **expr**: PromQL query
* **for: 5m**: Ø¨Ø§ÛŒØ¯ 5 Ø¯Ù‚ÛŒÙ‚Ù‡ continuous Ø¨Ø§Ø´Ø¯ (prevent flapping)
* **severity**: critical (page on-call engineer)
* **runbook**: link Ø¨Ù‡ documentation

**Alert Routing:**

```yaml
route:
  receiver: 'team-search'
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
  - match:
      severity: critical
    receiver: 'pagerduty'
  
  - match:
      severity: warning
    receiver: 'slack'
```

**ØªÙˆØ¶ÛŒØ­:**

* **group_by**: alerts Ù…Ø´Ø§Ø¨Ù‡ Ø±Ø§ group Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* **group_wait**: 30 Ø«Ø§Ù†ÛŒÙ‡ ØµØ¨Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (maybe more alerts come)
* **critical** â†’ PagerDuty (page engineer)
* **warning** â†’ Slack (notification only)


---

## 8. Security - Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ

### 8.1 Defense in Depth

```mermaidjs
graph TB
    subgraph "Security Layers"
        L1[Network - Firewall/WAF]
        L2[Application - Auth/AuthZ]
        L3[Data - Encryption]
        L4[Monitoring - Audit Logs]
        
        L1 --> L2
        L2 --> L3
        L3 --> L4
    end
```

**Ù‡Ø± layer Ù…Ø³ØªÙ‚Ù„ Ø§Ø³Øª:**

* Ø§Ú¯Ø± ÛŒÚ© layer bypass Ø´ÙˆØ¯ØŒ layers Ø¨Ø¹Ø¯ÛŒ Ù…Ø­Ø§ÙØ¸Øª Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
* Ø§ÛŒÙ† principle of least privilege Ø§Ø³Øª

### 8.2 JWT Authentication

**Ú†Ø±Ø§ JWTØŸ**

```
Traditional session:
  User â†’ Login â†’ Server stores session in DB
  User â†’ Request â†’ Server queries DB for session
  
  Problem: DB query per request (slow, doesn't scale)

JWT:
  User â†’ Login â†’ Server creates JWT token
  User â†’ Request â†’ Server verifies JWT signature (no DB!)
  
  Benefit: Stateless, fast, scalable
```

**JWT Structure:**

```
Header.Payload.Signature

eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.
eyJ1c2VyX2lkIjoiMTIzIiwicm9sZSI6InVzZXIifQ.
SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c
```

**Decoded:**

```json
// Header
{
  "alg": "HS256",
  "typ": "JWT"
}

// Payload
{
  "user_id": "123",
  "role": "user",
  "exp": 1735689600,  // Expiration timestamp
  "iat": 1735603200   // Issued at
}

// Signature
HMACSHA256(
  base64UrlEncode(header) + "." + base64UrlEncode(payload),
  secret
)
```

**Ú†Ú¯ÙˆÙ†Ù‡ Verify Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŸ**

```csharp
var tokenHandler = new JwtSecurityTokenHandler();
var key = Encoding.UTF8.GetBytes(_configuration["Jwt:Secret"]);

tokenHandler.ValidateToken(token, new TokenValidationParameters
{
    ValidateIssuerSigningKey = true,
    IssuerSigningKey = new SymmetricSecurityKey(key),
    ValidateIssuer = true,
    ValidIssuer = _configuration["Jwt:Issuer"],
    ValidateAudience = true,
    ValidAudience = _configuration["Jwt:Audience"],
    ValidateLifetime = true,
    ClockSkew = TimeSpan.Zero  // No tolerance for expired tokens
}, out SecurityToken validatedToken);
```

**Claims-Based Authorization:**

```csharp
[Authorize(Policy = "RequireAdvancedSearch")]
[HttpPost("api/v1/search/advanced")]
public async Task<IActionResult> AdvancedSearch(...)
{
    // Only users with "search:advanced" claim can access
}

// Policy definition
services.AddAuthorization(options =>
{
    options.AddPolicy("RequireAdvancedSearch", policy =>
        policy.RequireClaim("permission", "search:advanced"));
});
```

### 8.3 Rate Limiting - Advanced

**Ú†Ø±Ø§ Per-User Ùˆ Per-IPØŸ**

```
Per-User only:
  Attacker â†’ Creates 1000 accounts â†’ 100K requests
  
Per-IP only:
  Legitimate users behind NAT â†’ Blocked!
  
Both:
  Per-User: 100 req/min
  Per-IP: 10K req/min
  
  Balance between security and UX
```

**Sliding Window Algorithm:**

```csharp
public class SlidingWindowRateLimiter
{
    private readonly IDistributedCache _cache;
    private readonly int _limit = 100;
    private readonly TimeSpan _window = TimeSpan.FromMinutes(1);
    
    public async Task<bool> IsAllowedAsync(string key)
    {
        var now = DateTimeOffset.UtcNow.ToUnixTimeMilliseconds();
        var windowStart = now - (long)_window.TotalMilliseconds;
        
        // Redis sorted set: timestamp â†’ request
        var requests = await _cache.GetSortedSetRangeAsync(
            key,
            windowStart,
            now
        );
        
        if (requests.Count >= _limit)
            return false; // Rate limited
        
        // Add current request
        await _cache.SortedSetAddAsync(key, now, now);
        
        // Cleanup old requests
        await _cache.SortedSetRemoveRangeByScoreAsync(key, 0, windowStart);
        
        // Set expiration
        await _cache.ExpireAsync(key, _window);
        
        return true;
    }
}
```

**Redis Commands:**

```
ZADD rate_limit:user:123 1735689600000 1735689600000
ZCOUNT rate_limit:user:123 1735689540000 1735689600000
ZREMRANGEBYSCORE rate_limit:user:123 0 1735689540000
```

### 8.4 Input Validation

**Ú†Ø±Ø§ Validation Ù…Ù‡Ù… Ø§Ø³ØªØŸ**

```
SQL Injection:
  Input: "'; DROP TABLE Posts; --"
  Query: SELECT * FROM Posts WHERE caption LIKE '%'; DROP TABLE Posts; --%'
  
  Result: Table deleted!
```

**Defense:**

```csharp
public class SearchRequestValidator : AbstractValidator<SearchRequest>
{
    private static readonly string[] DANGEROUS_PATTERNS = new[]
    {
        "--", "/*", "*/", "xp_", "sp_", "exec", "execute",
        "drop", "delete", "truncate", "insert", "update",
        "<script", "javascript:", "onerror="
    };
    
    public SearchRequestValidator()
    {
        RuleFor(x => x.Query)
            .NotEmpty()
            .MaximumLength(500)
            .Must(NotContainDangerousPatterns)
            .WithMessage("Invalid characters in query");
    }
    
    private bool NotContainDangerousPatterns(string query)
    {
        if (string.IsNullOrWhiteSpace(query))
            return true;
        
        var lower = query.ToLowerInvariant();
        return !DANGEROUS_PATTERNS.Any(p => lower.Contains(p));
    }
}
```

**XSS (Cross-Site Scripting) Prevention:**

```csharp
// Encode output
public string EncodeCaption(string caption)
{
    return HttpUtility.HtmlEncode(caption);
}

// Input: "<script>alert('XSS')</script>"
// Output: "&lt;script&gt;alert('XSS')&lt;/script&gt;"
```

**Content Security Policy (CSP):**

```csharp
app.Use(async (context, next) =>
{
    context.Response.Headers.Add("Content-Security-Policy",
        "default-src 'self'; " +
        "script-src 'self' 'unsafe-inline'; " +
        "style-src 'self' 'unsafe-inline'; " +
        "img-src 'self' data: https:;");
    
    await next();
});
```

Ø§ÛŒÙ† Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯:

* Scripts ÙÙ‚Ø· Ø§Ø² Ù‡Ù…Ø§Ù† domain
* No inline scripts (except explicitly allowed)
* Images Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø² Ù‡Ø± HTTPS source Ø¨ÛŒØ§ÛŒÙ†Ø¯


---

## 9. Testing Strategy

### 9.1 Unit Tests

**Ú†Ø±Ø§ Unit TestingØŸ**

```
"Code without tests is broken by design."
```

**Ù…Ø«Ø§Ù„:**

```csharp
[Fact]
public void TextNormalizer_ShouldRemoveExtraWhitespace()
{
    // Arrange
    var normalizer = new TextNormalizer();
    var input = "  Hello    World  ";
    
    // Act
    var output = normalizer.Normalize(input);
    
    // Assert
    Assert.Equal("Hello World", output);
}

[Theory]
[InlineData("", "")]
[InlineData("  ", "")]
[InlineData("Hello", "Hello")]
[InlineData("  Hello  ", "Hello")]
public void TextNormalizer_VariousInputs(string input, string expected)
{
    var normalizer = new TextNormalizer();
    var output = normalizer.Normalize(input);
    Assert.Equal(expected, output);
}
```

**Coverage Target:**

```
Total: >80%
Critical paths: 100%
```

### 9.2 Integration Tests

**Elasticsearch Integration Test:**

```csharp
public class ElasticsearchIntegrationTests : IClassFixture<ElasticsearchFixture>
{
    private readonly IElasticClient _client;
    
    public ElasticsearchIntegrationTests(ElasticsearchFixture fixture)
    {
        _client = fixture.Client;
    }
    
    [Fact]
    public async Task Search_ShouldReturnResults()
    {
        // Arrange
        var testData = new[]
        {
            new Post { PostId = "1", Caption = "Beautiful sunset" },
            new Post { PostId = "2", Caption = "Amazing sunrise" }
        };
        
        // Index test data
        foreach (var post in testData)
        {
            await _client.IndexDocumentAsync(post);
        }
        
        // Wait for refresh
        await Task.Delay(1000);
        
        // Act
        var response = await _client.SearchAsync<Post>(s => s
            .Query(q => q.Match(m => m.Field(f => f.Caption).Query("sunset")))
        );
        
        // Assert
        Assert.Single(response.Documents);
        Assert.Equal("1", response.Documents.First().PostId);
    }
}
```

### 9.3 Load Testing

**Ú†Ø±Ø§ Load TestingØŸ**

```
"It works on my machine" â‰  "It works in production"
```

**k6 Load Test:**

```javascript
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
  stages: [
    { duration: '2m', target: 100 },    // Ramp-up to 100 users
    { duration: '5m', target: 100 },    // Stay at 100 users
    { duration: '2m', target: 1000 },   // Ramp-up to 1000 users
    { duration: '5m', target: 1000 },   // Stay at 1000 users
    { duration: '2m', target: 0 },      // Ramp-down to 0
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'], // 95% of requests < 500ms
    http_req_failed: ['rate<0.01'],   // Error rate < 1%
  },
};

export default function () {
  const queries = ['sunset', 'beach', 'food', 'travel', 'fashion'];
  const query = queries[Math.floor(Math.random() * queries.length)];
  
  const response = http.get(`https://api.example.com/search?q=${query}`);
  
  check(response, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
  
  sleep(1); // Think time
}
```

**Interpreting Results:**

```
âœ“ http_req_duration..............: avg=250ms p(95)=450ms
âœ“ http_req_failed................: 0.5%
âœ“ http_reqs......................: 10000/sec

Result: PASSED
```


---

## 10. Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ùˆ Recommendations

### 10.1 Key Takeaways


1. **Architecture**: Lambda architecture Ø¨Ø±Ø§ÛŒ balance Ø¨ÛŒÙ† real-time Ùˆ consistency
2. **Search**: Hybrid BM25 + Vector Ø¨Ø±Ø§ÛŒ best relevance
3. **Scale**: Sharding + Replication + Caching Ø¨Ø±Ø§ÛŒ performance
4. **Quality**: LTR + A/B Testing Ø¨Ø±Ø§ÛŒ continuous improvement
5. **Observability**: Metrics + Logs + Traces Ø¨Ø±Ø§ÛŒ visibility

### 10.2 Trade-offs

| Decision | Benefit | Cost |
|----|----|----|
| Elasticsearch | Fast search | Operational complexity |
| Vector Search | Semantic understanding | Compute cost |
| Real-time Indexing | Fresh content | Infrastructure cost |
| LTR | Better relevance | ML expertise needed |
| Multi-region | HA | 2Ã— cost |

### 10.3 Next Steps

**Week 1:**

* Team formation
* Infrastructure setup
* Kickoff meeting

**Month 1:**

* MVP: BM25 search
* Basic filters
* Performance: 1,000 QPS

**Month 3:**

* Vector search
* Hybrid ranking
* Performance: 5,000 QPS

**Month 6:**

* LTR
* Personalization
* Performance: 10,000 QPS

**Ø§ÛŒÙ† Ø¯Ø§Ú©ÛŒÙˆÙ…Ù†Øª ÛŒÚ© Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø³Øª. Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´ÛŒØ¯! ğŸš€**